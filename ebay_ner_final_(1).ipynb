{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d690ca5cd4e249b683d45befce263c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae0af4d36c8449cb96bf1defa9fa01c1",
              "IPY_MODEL_4b527a7d289b494193e82f811b0cd00d",
              "IPY_MODEL_bb0e5430031f4475a56e1ee1e562f680"
            ],
            "layout": "IPY_MODEL_88ac94d6882840ebb9db41bc4067e021"
          }
        },
        "ae0af4d36c8449cb96bf1defa9fa01c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_643760fe066040bca0beb18e1b969ef6",
            "placeholder": "​",
            "style": "IPY_MODEL_7165241ffbbb4f799d81a6a20186d39f",
            "value": "Loading weights: 100%"
          }
        },
        "4b527a7d289b494193e82f811b0cd00d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90fea72978644036a6fb5c98a60437ec",
            "max": 389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff97913cbd0b4c21892c4374edcaa99a",
            "value": 389
          }
        },
        "bb0e5430031f4475a56e1ee1e562f680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41b216ba9a5d4710a427c5bc51e58b8c",
            "placeholder": "​",
            "style": "IPY_MODEL_dca156b8ad294eeca956112c0c193bde",
            "value": " 389/389 [00:00&lt;00:00, 763.82it/s, Materializing param=bert.encoder.layer.23.output.dense.weight]"
          }
        },
        "88ac94d6882840ebb9db41bc4067e021": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "643760fe066040bca0beb18e1b969ef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7165241ffbbb4f799d81a6a20186d39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90fea72978644036a6fb5c98a60437ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff97913cbd0b4c21892c4374edcaa99a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41b216ba9a5d4710a427c5bc51e58b8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dca156b8ad294eeca956112c0c193bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa33b2d7367b44b78fca691d5ffab113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcee98aa5cd14628a1c3f9e981a47654",
              "IPY_MODEL_89d64371dcba4904a4088047a99ef88b",
              "IPY_MODEL_c7273c3c4f304ead888d0a926bf10e32"
            ],
            "layout": "IPY_MODEL_f0a1f5006b8e493dabe93b134b5b189c"
          }
        },
        "bcee98aa5cd14628a1c3f9e981a47654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2572d555837491abd5622d48a6922ca",
            "placeholder": "​",
            "style": "IPY_MODEL_28f4ba281170472a9fcf0ff3f78950f9",
            "value": "Loading weights: 100%"
          }
        },
        "89d64371dcba4904a4088047a99ef88b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f830fe523a9d4b4a8129841b108a08dd",
            "max": 389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f5b6edfc53a492db7c87399d2bdf431",
            "value": 389
          }
        },
        "c7273c3c4f304ead888d0a926bf10e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16fd4d7cb09b4cde9991371d34b79b92",
            "placeholder": "​",
            "style": "IPY_MODEL_9a58c3c1d0304333a45ca95bfeb62baf",
            "value": " 389/389 [00:01&lt;00:00, 444.28it/s, Materializing param=bert.encoder.layer.23.output.dense.weight]"
          }
        },
        "f0a1f5006b8e493dabe93b134b5b189c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2572d555837491abd5622d48a6922ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28f4ba281170472a9fcf0ff3f78950f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f830fe523a9d4b4a8129841b108a08dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f5b6edfc53a492db7c87399d2bdf431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16fd4d7cb09b4cde9991371d34b79b92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a58c3c1d0304333a45ca95bfeb62baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c6df1358dfd479b91e8914772d7be22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1181e729eb34c36bc6502aecd89976d",
              "IPY_MODEL_168ea950f6694f8c8608a38cf8894155",
              "IPY_MODEL_6a000f5e72a24af5af5f61fbd1dbc3b9"
            ],
            "layout": "IPY_MODEL_e94c2b6244eb4da8adc44cfcc0e0ab2b"
          }
        },
        "e1181e729eb34c36bc6502aecd89976d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5706873736074723b535a66f89312478",
            "placeholder": "​",
            "style": "IPY_MODEL_90d5b91c1503470eb177e1575bebe79d",
            "value": "Loading weights: 100%"
          }
        },
        "168ea950f6694f8c8608a38cf8894155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34dcfbc0a8bc429babba1fc5f41d2828",
            "max": 389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d898277b229a407c8427adcef9946d68",
            "value": 389
          }
        },
        "6a000f5e72a24af5af5f61fbd1dbc3b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9751b8ab6b3f45dcbbe31287d848635c",
            "placeholder": "​",
            "style": "IPY_MODEL_717fd59258144a1ea159aed117072ed5",
            "value": " 389/389 [00:01&lt;00:00, 396.20it/s, Materializing param=bert.encoder.layer.23.output.dense.weight]"
          }
        },
        "e94c2b6244eb4da8adc44cfcc0e0ab2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5706873736074723b535a66f89312478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90d5b91c1503470eb177e1575bebe79d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34dcfbc0a8bc429babba1fc5f41d2828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d898277b229a407c8427adcef9946d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9751b8ab6b3f45dcbbe31287d848635c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "717fd59258144a1ea159aed117072ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6aad1166bcdf4d359ee9c1e821d6c860": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4f7d43545d4426ba4f0588c22505617",
              "IPY_MODEL_591b171fe4944794a3d037cbb90de23f",
              "IPY_MODEL_3ddee91a0a8c49829899d8bc97fc6b08"
            ],
            "layout": "IPY_MODEL_cec6b644573d4fb2b1c792edc9941e00"
          }
        },
        "e4f7d43545d4426ba4f0588c22505617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed931fb03e0d4c68b4b77990f429dffe",
            "placeholder": "​",
            "style": "IPY_MODEL_08b6bdba996e4f5f8a29c0ee1369dc05",
            "value": "Loading weights: 100%"
          }
        },
        "591b171fe4944794a3d037cbb90de23f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f26f6df1fa94413086018d08d53ae611",
            "max": 389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46bc01d0357942a38177598c0861f436",
            "value": 389
          }
        },
        "3ddee91a0a8c49829899d8bc97fc6b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_308a8fdf924441a1a742d50624c47475",
            "placeholder": "​",
            "style": "IPY_MODEL_4c78e5af83754ce987cbadd1c8b01677",
            "value": " 389/389 [00:00&lt;00:00, 573.89it/s, Materializing param=bert.encoder.layer.23.output.dense.weight]"
          }
        },
        "cec6b644573d4fb2b1c792edc9941e00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed931fb03e0d4c68b4b77990f429dffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b6bdba996e4f5f8a29c0ee1369dc05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f26f6df1fa94413086018d08d53ae611": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46bc01d0357942a38177598c0861f436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "308a8fdf924441a1a742d50624c47475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c78e5af83754ce987cbadd1c8b01677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anujac1206/AppDemo/blob/main/ebay_ner_final_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLb71sMQCiFI"
      },
      "source": [
        "# eBay NER Competition - Complete Pipeline\n",
        "## BEFORE RUNNING: Runtime > Change runtime type > T4 GPU\n",
        "\n",
        "### Your files:\n",
        "- `Listing_Titles.tsv.gz` — 2M titles (records 1-2,000,000). Train=1-5000, Quiz=5001-30000\n",
        "- `Tagged_Titles_Train.tsv.gz` — training labels (records 1-5000, one row per token)\n",
        "\n",
        "### What this notebook does:\n",
        "1. Loads and parses your exact file format\n",
        "2. Trains gbert-large with 3-fold CV\n",
        "3. Tunes threshold for F-beta=0.2 (precision-heavy)\n",
        "4. Generates submission for Quiz records 5001-30000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr-wY3dbCiFK"
      },
      "source": [
        "## CELL 1 — Install & Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XfJ09vryVzj",
        "outputId": "bb0912c5-3fd6-480e-9107-a21489a7ba0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.10.0+cpu\n",
            "Uninstalling torch-2.10.0+cpu:\n",
            "  Successfully uninstalled torch-2.10.0+cpu\n",
            "Found existing installation: torchvision 0.25.0+cpu\n",
            "Uninstalling torchvision-0.25.0+cpu:\n",
            "  Successfully uninstalled torchvision-0.25.0+cpu\n",
            "Found existing installation: torchaudio 2.10.0+cpu\n",
            "Uninstalling torchaudio-2.10.0+cpu:\n",
            "  Successfully uninstalled torchaudio-2.10.0+cpu\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.1/819.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY0af-1qzXV2",
        "outputId": "c5702a2a-5c5d-4ec4-8662-fea9ddc289d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-295/690118984.py\", line 1, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.2.2+cu118\n",
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#when gpu not available\n",
        "DEVICE = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "NlLsB1y2rzpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ZKt_p2plCiFK",
        "outputId": "293c40aa-9908-4361-ef67-8353db4b7858"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "NO GPU! Go to Runtime > Change runtime type > T4 GPU and re-run.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-295/3757255548.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NO GPU! Go to Runtime > Change runtime type > T4 GPU and re-run.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Memory:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_memory\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: NO GPU! Go to Runtime > Change runtime type > T4 GPU and re-run."
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets -q\n",
        "\n",
        "import os, csv, gzip, torch, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError('NO GPU! Go to Runtime > Change runtime type > T4 GPU and re-run.')\n",
        "print('GPU:', torch.cuda.get_device_name(0))\n",
        "print('Memory:', round(torch.cuda.get_device_properties(0).total_memory/1e9, 1), 'GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUp7C2aqCiFL"
      },
      "source": [
        "## CELL 2 — Mount Drive & Set Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sJT_HVyCiFL",
        "outputId": "697dbe22-b894-48b6-ec12-0d6a5102055c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "OK: /content/drive/MyDrive/ebay_ner/Listing_Titles.tsv\n",
            "OK: /content/drive/MyDrive/ebay_ner/Tagged_Titles_Train.tsv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ================================================================\n",
        "# EDIT THESE to match where you put the files in Google Drive\n",
        "# ================================================================\n",
        "BASE_DIR      = '/content/drive/MyDrive/ebay_ner'   # folder containing your files\n",
        "LISTING_FILE  = f'{BASE_DIR}/Listing_Titles.tsv'\n",
        "TRAIN_FILE    = f'{BASE_DIR}/Tagged_Titles_Train.tsv'\n",
        "OUTPUT_DIR    = f'{BASE_DIR}/outputs'\n",
        "# ================================================================\n",
        "\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Check files exist\n",
        "for f in [LISTING_FILE, TRAIN_FILE]:\n",
        "    exists = os.path.exists(f)\n",
        "    # Also try without .gz in case already decompressed\n",
        "    if not exists:\n",
        "        f_nogz = f.replace('.gz', '')\n",
        "        exists = os.path.exists(f_nogz)\n",
        "        if exists:\n",
        "            print(f'Found without .gz: {f_nogz}')\n",
        "    print(f'{\"OK\" if exists else \"MISSING\"}: {f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hv15a4LCiFL"
      },
      "source": [
        "## CELL 3 — Load Training Data\n",
        "Format: Record Number, Category Id, Title, Token, Tag (5 columns, one row per token)\n",
        "Empty Tag = continuation of previous entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wq1YX13CiFL",
        "outputId": "f63756a1-68bb-4375-dbbc-9f75bb5c2c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train columns: ['record_number', 'category', 'title', 'token', 'tag']\n",
            "Rows: 56812\n",
            "    record_number  category                                                                             title             token                          tag\n",
            "0               1         2  MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803              MINI    Kompatible_Fahrzeug_Marke\n",
            "1               1         2  MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803               1.6  Kompatibles_Fahrzeug_Modell\n",
            "2               1         2  MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803           W10B16A             Herstellernummer\n",
            "3               1         2  MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803           W11B16A                             \n",
            "4               1         2  MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803               R50                             \n",
            "5               1         2  MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803  Steuerkettensatz                   Produktart\n",
            "6               1         2  MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803       11311485400             Herstellernummer\n",
            "7               1         2  MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803       Steuerkette    Im_Lieferumfang_Enthalten\n",
            "8               1         2  MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803              FEBI                   Hersteller\n",
            "9               1         2  MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803             31803             Herstellernummer\n",
            "10              2         1           ATE Power Disc Bremsenset Mercedes SLC + SLK Vorne 295MM + Hinten 300MM               ATE                   Hersteller\n",
            "11              2         1           ATE Power Disc Bremsenset Mercedes SLC + SLK Vorne 295MM + Hinten 300MM             Power                       Modell\n",
            "12              2         1           ATE Power Disc Bremsenset Mercedes SLC + SLK Vorne 295MM + Hinten 300MM              Disc                             \n",
            "13              2         1           ATE Power Disc Bremsenset Mercedes SLC + SLK Vorne 295MM + Hinten 300MM        Bremsenset                   Produktart\n",
            "14              2         1           ATE Power Disc Bremsenset Mercedes SLC + SLK Vorne 295MM + Hinten 300MM          Mercedes    Kompatible_Fahrzeug_Marke\n"
          ]
        }
      ],
      "source": [
        "def load_train_file(filepath):\n",
        "    \"\"\"\n",
        "    Loads Tagged_Titles_Train.tsv.gz\n",
        "    Columns: Record Number, Category Id, Title, Token, Tag\n",
        "    Empty Tag = continuation token (belongs to previous entity)\n",
        "    \"\"\"\n",
        "    # Handle both .gz and plain .tsv\n",
        "    if filepath.endswith('.gz'):\n",
        "        opener = lambda: gzip.open(filepath, 'rt', encoding='utf-8')\n",
        "    else:\n",
        "        opener = lambda: open(filepath, 'r', encoding='utf-8')\n",
        "\n",
        "    # CRITICAL pandas settings from the competition doc:\n",
        "    # keep_default_na=False, na_values=None  → empty string stays as empty string, not NaN\n",
        "    with opener() as f:\n",
        "        df = pd.read_csv(\n",
        "            f,\n",
        "            sep='\\t',\n",
        "            keep_default_na=False,   # CRITICAL: empty tag stays '', not NaN\n",
        "            na_values=None,          # CRITICAL: nothing becomes NaN\n",
        "            quoting=0,               # csv.QUOTE_MINIMAL — handles CSV-style quoting\n",
        "            header=0                 # first row is header\n",
        "        )\n",
        "\n",
        "    # Normalise column names (strip spaces, lowercase)\n",
        "    df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]\n",
        "    print('Train columns:', df.columns.tolist())\n",
        "    print(f'Rows: {len(df)}')\n",
        "    print(df.head(15).to_string())\n",
        "    return df\n",
        "\n",
        "df_train_raw = load_train_file(TRAIN_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju8XlyroCiFL"
      },
      "source": [
        "## CELL 4 — Fix Column Names (Edit if needed)\n",
        "Look at the output above and confirm column names. Edit this cell if they differ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDQZQG4VCiFL",
        "outputId": "0eb50ce8-da4b-49f6-d1fa-35c0f6135509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using columns:\n",
            "  Record:   record_number\n",
            "  Category: category\n",
            "  Title:    title\n",
            "  Token:    token\n",
            "  Tag:      tag\n",
            "\n",
            "Unique tags (31): ['', 'Anwendung', 'Anzahl_Der_Einheiten', 'Besonderheiten', 'Breite', 'Bremsscheiben-Aussendurchmesser', 'Bremsscheibenart', 'Einbauposition', 'Farbe', 'Größe', 'Hersteller', 'Herstellernummer', 'Herstellungsland_Und_-Region', 'Im_Lieferumfang_Enthalten', 'Kompatible_Fahrzeug_Marke', 'Kompatibles_Fahrzeug_Jahr', 'Kompatibles_Fahrzeug_Modell', 'Länge', 'Material', 'Maßeinheit', 'Menge', 'Modell', 'O', 'Oberflächenbeschaffenheit', 'Oe/Oem_Referenznummer(N)', 'Produktart', 'Produktlinie', 'SAE_Viskosität', 'Stärke', 'Technologie', 'Zähnezahl']\n"
          ]
        }
      ],
      "source": [
        "# After seeing the printout above, set the correct column names here:\n",
        "# Common variations the competition uses:\n",
        "# 'record_number' or 'record number' or 'record_id'\n",
        "# 'category_id' or 'category id'\n",
        "# 'title', 'token', 'tag'\n",
        "\n",
        "# Auto-detect columns (works for most cases)\n",
        "cols = df_train_raw.columns.tolist()\n",
        "\n",
        "# Find each column by position or name\n",
        "COL_RECORD   = cols[0]   # first column  = record number\n",
        "COL_CATEGORY = cols[1]   # second column = category id\n",
        "COL_TITLE    = cols[2]   # third column  = title\n",
        "COL_TOKEN    = cols[3]   # fourth column = token\n",
        "COL_TAG      = cols[4]   # fifth column  = tag\n",
        "\n",
        "print(f'Using columns:')\n",
        "print(f'  Record:   {COL_RECORD}')\n",
        "print(f'  Category: {COL_CATEGORY}')\n",
        "print(f'  Title:    {COL_TITLE}')\n",
        "print(f'  Token:    {COL_TOKEN}')\n",
        "print(f'  Tag:      {COL_TAG}')\n",
        "\n",
        "# Show unique non-empty tags\n",
        "all_raw_tags = df_train_raw[COL_TAG].unique()\n",
        "print(f'\\nUnique tags ({len(all_raw_tags)}):', sorted(all_raw_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvpeV6WACiFM"
      },
      "source": [
        "## CELL 5 — Parse Into Training Examples\n",
        "This handles the empty-tag continuation logic correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISVYXoneCiFM",
        "outputId": "627025b9-aa69-4581-d90e-98086a621c4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed 5000 training examples\n",
            "\n",
            "--- Example 1 (should match Annexure Example 1) ---\n",
            "Title: MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803\n",
            "  MINI                           B-Kompatible_Fahrzeug_Marke\n",
            "  1.6                            B-Kompatibles_Fahrzeug_Modell\n",
            "  W10B16A                        B-Herstellernummer\n",
            "  W11B16A                        I-Herstellernummer\n",
            "  R50                            I-Herstellernummer\n",
            "  Steuerkettensatz               B-Produktart\n",
            "  11311485400                    B-Herstellernummer\n",
            "  Steuerkette                    B-Im_Lieferumfang_Enthalten\n",
            "  FEBI                           B-Hersteller\n",
            "  31803                          B-Herstellernummer\n",
            "\n",
            "Total labels: 48\n",
            "Labels: ['O', 'B-Anwendung', 'B-Anzahl_Der_Einheiten', 'B-Besonderheiten', 'B-Breite', 'B-Bremsscheiben-Aussendurchmesser', 'B-Bremsscheibenart', 'B-Einbauposition', 'B-Farbe', 'B-Größe', 'B-Hersteller', 'B-Herstellernummer', 'B-Herstellungsland_Und_-Region', 'B-Im_Lieferumfang_Enthalten', 'B-Kompatible_Fahrzeug_Marke', 'B-Kompatibles_Fahrzeug_Jahr', 'B-Kompatibles_Fahrzeug_Modell', 'B-Länge', 'B-Material', 'B-Maßeinheit', 'B-Menge', 'B-Modell', 'B-Oberflächenbeschaffenheit', 'B-Oe/Oem_Referenznummer(N)', 'B-Produktart', 'B-Produktlinie', 'B-SAE_Viskosität', 'B-Stärke', 'B-Technologie', 'B-Zähnezahl', 'I-Anwendung', 'I-Anzahl_Der_Einheiten', 'I-Besonderheiten', 'I-Bremsscheiben-Aussendurchmesser', 'I-Bremsscheibenart', 'I-Einbauposition', 'I-Größe', 'I-Hersteller', 'I-Herstellernummer', 'I-Im_Lieferumfang_Enthalten', 'I-Kompatible_Fahrzeug_Marke', 'I-Kompatibles_Fahrzeug_Jahr', 'I-Kompatibles_Fahrzeug_Modell', 'I-Modell', 'I-Oe/Oem_Referenznummer(N)', 'I-Produktart', 'I-Produktlinie', 'I-SAE_Viskosität']\n"
          ]
        }
      ],
      "source": [
        "def parse_train_data(df):\n",
        "    \"\"\"\n",
        "    Converts the raw TSV dataframe into list of examples.\n",
        "    Each example: {record_id, category_id, tokens, tags}\n",
        "\n",
        "    Handles empty tag = continuation of previous entity.\n",
        "    Empty tag → same tag as previous row (I- version for BIO format).\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "\n",
        "    for record_id, group in df.groupby(COL_RECORD, sort=False):\n",
        "        group = group.reset_index(drop=True)\n",
        "        tokens = group[COL_TOKEN].tolist()\n",
        "        raw_tags = group[COL_TAG].tolist()  # some are '', some are tag names\n",
        "        category_id = str(group[COL_CATEGORY].iloc[0])\n",
        "        title = group[COL_TITLE].iloc[0]\n",
        "\n",
        "        # Convert to BIO tags handling empty = continuation\n",
        "        bio_tags = []\n",
        "        current_entity = None  # tracks what entity we're inside\n",
        "\n",
        "        for raw_tag in raw_tags:\n",
        "            if raw_tag == '':  # CONTINUATION TOKEN\n",
        "                if current_entity is not None and current_entity != 'O':\n",
        "                    bio_tags.append(f'I-{current_entity}')\n",
        "                else:\n",
        "                    bio_tags.append('O')\n",
        "            else:  # NEW TAG\n",
        "                if raw_tag == 'O':\n",
        "                    bio_tags.append('O')\n",
        "                    current_entity = 'O'\n",
        "                else:\n",
        "                    bio_tags.append(f'B-{raw_tag}')\n",
        "                    current_entity = raw_tag\n",
        "\n",
        "        assert len(tokens) == len(bio_tags), \\\n",
        "            f'Length mismatch in record {record_id}: {len(tokens)} tokens vs {len(bio_tags)} tags'\n",
        "\n",
        "        examples.append({\n",
        "            'record_id':   str(record_id),\n",
        "            'category_id': category_id,\n",
        "            'title':       title,\n",
        "            'tokens':      tokens,\n",
        "            'tags':        bio_tags\n",
        "        })\n",
        "\n",
        "    return examples\n",
        "\n",
        "\n",
        "train_examples = parse_train_data(df_train_raw)\n",
        "\n",
        "print(f'Parsed {len(train_examples)} training examples')\n",
        "print('\\n--- Example 1 (should match Annexure Example 1) ---')\n",
        "ex = train_examples[0]\n",
        "print('Title:', ex['title'])\n",
        "for tok, tag in zip(ex['tokens'], ex['tags']):\n",
        "    print(f'  {tok:30s} {tag}')\n",
        "\n",
        "# Build label set\n",
        "all_tags = set()\n",
        "for ex in train_examples:\n",
        "    all_tags.update(ex['tags'])\n",
        "\n",
        "sorted_labels = ['O'] + sorted(t for t in all_tags if t != 'O')\n",
        "label2id = {t: i for i, t in enumerate(sorted_labels)}\n",
        "id2label  = {i: t for t, i in label2id.items()}\n",
        "NUM_LABELS = len(sorted_labels)\n",
        "\n",
        "print(f'\\nTotal labels: {NUM_LABELS}')\n",
        "print('Labels:', sorted_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build allowed aspect names per category from training data\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "allowed_aspects_per_category = defaultdict(set)\n",
        "\n",
        "for ex in train_examples:\n",
        "    cat = ex['category_id']\n",
        "    spans = bio_to_spans(ex['tokens'], ex['tags'])\n",
        "    for aspect_name, _ in spans:\n",
        "        if aspect_name != 'O':\n",
        "            allowed_aspects_per_category[cat].add(aspect_name)\n",
        "\n",
        "print(\"Allowed aspects per category:\")\n",
        "for cat, aspects in allowed_aspects_per_category.items():\n",
        "    print(f\"Category {cat}:\")\n",
        "    for a in sorted(aspects):\n",
        "        print(\"  \", a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkdx2O-qG--M",
        "outputId": "acab1baf-4c46-429a-fc3d-9284d4c34911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allowed aspects per category:\n",
            "Category 2:\n",
            "   Anwendung\n",
            "   Anzahl_Der_Einheiten\n",
            "   Besonderheiten\n",
            "   Breite\n",
            "   Einbauposition\n",
            "   Größe\n",
            "   Hersteller\n",
            "   Herstellernummer\n",
            "   Im_Lieferumfang_Enthalten\n",
            "   Kompatible_Fahrzeug_Marke\n",
            "   Kompatibles_Fahrzeug_Jahr\n",
            "   Kompatibles_Fahrzeug_Modell\n",
            "   Länge\n",
            "   Maßeinheit\n",
            "   Menge\n",
            "   Modell\n",
            "   Oe/Oem_Referenznummer(N)\n",
            "   Produktart\n",
            "   SAE_Viskosität\n",
            "   Zähnezahl\n",
            "Category 1:\n",
            "   Anzahl_Der_Einheiten\n",
            "   Besonderheiten\n",
            "   Bremsscheiben-Aussendurchmesser\n",
            "   Bremsscheibenart\n",
            "   Einbauposition\n",
            "   Farbe\n",
            "   Größe\n",
            "   Hersteller\n",
            "   Herstellernummer\n",
            "   Herstellungsland_Und_-Region\n",
            "   Im_Lieferumfang_Enthalten\n",
            "   Kompatible_Fahrzeug_Marke\n",
            "   Kompatibles_Fahrzeug_Jahr\n",
            "   Kompatibles_Fahrzeug_Modell\n",
            "   Material\n",
            "   Maßeinheit\n",
            "   Modell\n",
            "   Oberflächenbeschaffenheit\n",
            "   Oe/Oem_Referenznummer(N)\n",
            "   Produktart\n",
            "   Produktlinie\n",
            "   Stärke\n",
            "   Technologie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_exs, val_exs = train_test_split(\n",
        "    train_examples,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Train:\", len(train_exs))\n",
        "print(\"Val:\", len(val_exs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EES-DZIRIZNI",
        "outputId": "edd43da8-aaad-4f65-d596-91ba976af878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 4500\n",
            "Val: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9d1Gl9NCiFM"
      },
      "source": [
        "## CELL 6 — Load Listing Titles (for Quiz/Test set)\n",
        "Listing_Titles.tsv.gz: Record Number, Category Id, Title (3 columns)\n",
        "Quiz = records 5001-30000, Test = disclosed later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R97HiZauCiFM",
        "outputId": "24288649-ded5-4996-c2d3-e198207ff69d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing columns: ['record_number', 'category', 'title']\n",
            "Total records: 2000000\n",
            "   record_number  category                                                                             title\n",
            "0              1         2  MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803\n",
            "1              2         1           ATE Power Disc Bremsenset Mercedes SLC + SLK Vorne 295MM + Hinten 300MM\n",
            "2              3         1        Textar Bremsscheiben + Bremsbeläge hinten für Fiat Freemont Lancia Voyager\n",
            "\n",
            "Quiz records: 25000\n",
            "Quiz examples ready: 25000\n",
            "Sample quiz example: {'record_id': '5001', 'category_id': '1', 'title': 'OPEL ASTRA H 1.7 CDTI-SET 2 Bremsscheiben 4 Beläge VA', 'tokens': ['OPEL', 'ASTRA', 'H', '1.7', 'CDTI-SET', '2', 'Bremsscheiben', '4', 'Beläge', 'VA']}\n"
          ]
        }
      ],
      "source": [
        "def load_listing_file(filepath):\n",
        "    if filepath.endswith('.gz'):\n",
        "        opener = lambda: gzip.open(filepath, 'rt', encoding='utf-8')\n",
        "    else:\n",
        "        opener = lambda: open(filepath, 'r', encoding='utf-8')\n",
        "\n",
        "    with opener() as f:\n",
        "        df = pd.read_csv(\n",
        "            f,\n",
        "            sep='\\t',\n",
        "            keep_default_na=False,\n",
        "            na_values=None,\n",
        "            quoting=0,\n",
        "            header=0\n",
        "        )\n",
        "    df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]\n",
        "    print('Listing columns:', df.columns.tolist())\n",
        "    print(f'Total records: {len(df)}')\n",
        "    print(df.head(3).to_string())\n",
        "    return df\n",
        "\n",
        "df_listing = load_listing_file(LISTING_FILE)\n",
        "\n",
        "# Auto-detect column names\n",
        "listing_cols = df_listing.columns.tolist()\n",
        "LIST_COL_RECORD   = listing_cols[0]\n",
        "LIST_COL_CATEGORY = listing_cols[1]\n",
        "LIST_COL_TITLE    = listing_cols[2]\n",
        "\n",
        "# Extract Quiz set: records 5001-30000\n",
        "# Record numbers start at 1 and are in the first column\n",
        "quiz_df = df_listing[\n",
        "    (df_listing[LIST_COL_RECORD] >= 5001) &\n",
        "    (df_listing[LIST_COL_RECORD] <= 30000)\n",
        "].copy()\n",
        "\n",
        "print(f'\\nQuiz records: {len(quiz_df)}')\n",
        "\n",
        "# Build quiz examples (whitespace tokenization only - competition rule)\n",
        "quiz_examples = []\n",
        "for _, row in quiz_df.iterrows():\n",
        "    title = str(row[LIST_COL_TITLE])\n",
        "    tokens = title.split()  # WHITESPACE ONLY - competition rule, no other processing\n",
        "    quiz_examples.append({\n",
        "        'record_id':   str(int(row[LIST_COL_RECORD])),\n",
        "        'category_id': str(int(row[LIST_COL_CATEGORY])),\n",
        "        'title':       title,\n",
        "        'tokens':      tokens\n",
        "    })\n",
        "\n",
        "print(f'Quiz examples ready: {len(quiz_examples)}')\n",
        "print('Sample quiz example:', quiz_examples[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEwFedruCiFM"
      },
      "source": [
        "## CELL 7 — Tokenizer & Subword Alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjEtTAk4CiFM",
        "outputId": "a39b8669-6df9-4dfe-c7d9-342326f24278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: deepset/gbert-large\n",
            "input_ids shape: torch.Size([128])\n",
            "labels shape:    torch.Size([128])\n",
            "Tokenizer alignment OK\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ================================================================\n",
        "# MODEL CHOICE (try in order if you get OOM errors):\n",
        "# 1. 'deepset/gbert-large'      — best quality, needs ~13GB VRAM\n",
        "# 2. 'deepset/gbert-base'       — good quality, safe on T4\n",
        "# 3. 'bert-base-german-cased'   — fallback\n",
        "# ================================================================\n",
        "MODEL_NAME = 'deepset/gbert-large'\n",
        "MAX_LEN = 128  # titles are short, 128 is more than enough\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f'Loaded: {MODEL_NAME}')\n",
        "\n",
        "def tokenize_and_align_labels(example):\n",
        "    \"\"\"\n",
        "    BERT uses subword tokenization. 'Bremsscheibe' might split into ['Brems', '##scheibe'].\n",
        "    We label only the FIRST subtoken of each word; others get -100 (ignored in loss).\n",
        "    \"\"\"\n",
        "    enc = tokenizer(\n",
        "        example['tokens'],\n",
        "        is_split_into_words=True,  # tells tokenizer input is already word-split\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    word_ids = enc.word_ids(batch_index=0)\n",
        "    aligned = []\n",
        "    prev_word_id = None\n",
        "    for wid in word_ids:\n",
        "        if wid is None:              # [CLS], [SEP], [PAD]\n",
        "            aligned.append(-100)\n",
        "        elif wid != prev_word_id:    # first subtoken of a word → use its label\n",
        "            aligned.append(label2id[example['tags'][wid]])\n",
        "        else:                        # continuation subtoken → ignore\n",
        "            aligned.append(-100)\n",
        "        prev_word_id = wid\n",
        "\n",
        "    return {\n",
        "        'input_ids':      enc['input_ids'].squeeze(),\n",
        "        'attention_mask': enc['attention_mask'].squeeze(),\n",
        "        'labels':         torch.tensor(aligned)\n",
        "    }\n",
        "\n",
        "# Verify alignment works on first example\n",
        "sample = tokenize_and_align_labels(train_examples[0])\n",
        "print('input_ids shape:', sample['input_ids'].shape)\n",
        "print('labels shape:   ', sample['labels'].shape)\n",
        "print('Tokenizer alignment OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3h6kV6aCiFM"
      },
      "source": [
        "## CELL 8 — Dataset & Model Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "tHiItE9nCiFM",
        "outputId": "1ad94d1c-f103-42f1-9788-a7deac9060ed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'MODEL_NAME' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-295/318721659.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Test model loads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fresh_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Model OK: {MODEL_NAME} ({params:.0f}M params)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-295/318721659.py\u001b[0m in \u001b[0;36mload_fresh_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_fresh_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     model = AutoModelForTokenClassification.from_pretrained(\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mid2label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MODEL_NAME' is not defined"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, examples):\n",
        "        print(f'Building dataset from {len(examples)} examples...')\n",
        "        self.data = [tokenize_and_align_labels(ex) for ex in examples]\n",
        "        print('Done.')\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "def load_fresh_model():\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=NUM_LABELS,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Test model loads\n",
        "DEVICE = torch.device('cuda')\n",
        "m = load_fresh_model()\n",
        "params = sum(p.numel() for p in m.parameters()) / 1e6\n",
        "print(f'Model OK: {MODEL_NAME} ({params:.0f}M params)')\n",
        "del m\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload saved fold models (no retraining)\n",
        "\n",
        "fold_models = []\n",
        "DEVICE = torch.device(\"cuda\")\n",
        "\n",
        "for fold_num in range(1, 4):  # change if N_FOLDS different\n",
        "    model = load_fresh_model().to(DEVICE)\n",
        "    path = f'{OUTPUT_DIR}/fold{fold_num}_best.pt'\n",
        "    model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "    model.eval()\n",
        "    fold_models.append(model)\n",
        "    print(f'Loaded fold {fold_num}')\n",
        "\n",
        "print(\"All folds loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "qy3qXHhg0gHL",
        "outputId": "592c76d6-74d4-4976-d0b1-092a14e68f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_fresh_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-295/2474386032.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# change if N_FOLDS different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fresh_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{OUTPUT_DIR}/fold{fold_num}_best.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_fresh_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MUKpQSiCiFN"
      },
      "source": [
        "\\**bold text**## CELL 9 — Training Loop (One Fold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hmn8vdTCiFN"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def train_fold(train_exs, val_exs, fold_num, num_epochs=4, batch_size=16):\n",
        "\n",
        "    print(f'\\n========== FOLD {fold_num} ==========')\n",
        "    print(f'Train: {len(train_exs)} | Val: {len(val_exs)}')\n",
        "\n",
        "    if len(train_exs) == 0 or len(val_exs) == 0:\n",
        "        raise ValueError(\"train_exs or val_exs is empty\")\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", DEVICE)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        NERDataset(train_exs),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    val_dl = DataLoader(\n",
        "        NERDataset(val_exs),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    print(\"Train batches:\", len(train_dl))\n",
        "    print(\"Val batches:\", len(val_dl))\n",
        "\n",
        "    model = load_fresh_model().to(DEVICE)\n",
        "\n",
        "    lr = 1e-5 if 'large' in MODEL_NAME else 2e-5\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    total_steps = len(train_dl) * num_epochs\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    use_amp = torch.cuda.is_available()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    save_path = f'{OUTPUT_DIR}/fold{fold_num}_best.pt'\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for step, batch in enumerate(train_dl):\n",
        "\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                loss = model(**batch).loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if step % 30 == 0:\n",
        "                print(f'  Ep{epoch+1} step{step}/{len(train_dl)} loss={loss.item():.4f}')\n",
        "\n",
        "        # --- Validate ---\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dl:\n",
        "                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "                with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                    val_loss += model(**batch).loss.item()\n",
        "\n",
        "        avg_train = total_loss / len(train_dl)\n",
        "        avg_val   = val_loss   / len(val_dl)\n",
        "\n",
        "        print(f'  >> Epoch {epoch+1}: train_loss={avg_train:.4f}  val_loss={avg_val:.4f}')\n",
        "\n",
        "        if avg_val < best_val_loss:\n",
        "            best_val_loss = avg_val\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f'  ** Saved best (val_loss={best_val_loss:.4f})')\n",
        "\n",
        "    model.load_state_dict(torch.load(save_path, map_location=DEVICE))\n",
        "    model.eval()\n",
        "\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_fold(train_exs, val_exs, fold_num=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d690ca5cd4e249b683d45befce263c2f",
            "ae0af4d36c8449cb96bf1defa9fa01c1",
            "4b527a7d289b494193e82f811b0cd00d",
            "bb0e5430031f4475a56e1ee1e562f680",
            "88ac94d6882840ebb9db41bc4067e021",
            "643760fe066040bca0beb18e1b969ef6",
            "7165241ffbbb4f799d81a6a20186d39f",
            "90fea72978644036a6fb5c98a60437ec",
            "ff97913cbd0b4c21892c4374edcaa99a",
            "41b216ba9a5d4710a427c5bc51e58b8c",
            "dca156b8ad294eeca956112c0c193bde"
          ]
        },
        "id": "31HGnIRtHqg7",
        "outputId": "34d3d9b5-a7b1-4572-99ad-a3764895a2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== FOLD 1 ==========\n",
            "Train: 4500 | Val: 500\n",
            "Using device: cuda\n",
            "Building dataset from 4500 examples...\n",
            "Done.\n",
            "Building dataset from 500 examples...\n",
            "Done.\n",
            "Train batches: 282\n",
            "Val batches: 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d690ca5cd4e249b683d45befce263c2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertForTokenClassification LOAD REPORT from: deepset/gbert-large\n",
            "Key                                        | Status     | \n",
            "-------------------------------------------+------------+-\n",
            "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
            "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
            "cls.predictions.bias                       | UNEXPECTED | \n",
            "bert.pooler.dense.bias                     | UNEXPECTED | \n",
            "cls.seq_relationship.bias                  | UNEXPECTED | \n",
            "cls.seq_relationship.weight                | UNEXPECTED | \n",
            "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
            "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
            "bert.pooler.dense.weight                   | UNEXPECTED | \n",
            "classifier.bias                            | MISSING    | \n",
            "classifier.weight                          | MISSING    | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
            "/tmp/ipython-input-623/131612518.py:48: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/tmp/ipython-input-623/131612518.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-623/131612518.py:71: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  scheduler.step()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Ep1 step0/282 loss=4.2598\n",
            "  Ep1 step30/282 loss=3.4358\n",
            "  Ep1 step60/282 loss=1.5059\n",
            "  Ep1 step90/282 loss=0.8992\n",
            "  Ep1 step120/282 loss=0.5938\n",
            "  Ep1 step150/282 loss=0.4702\n",
            "  Ep1 step180/282 loss=0.5627\n",
            "  Ep1 step210/282 loss=0.4098\n",
            "  Ep1 step240/282 loss=0.3302\n",
            "  Ep1 step270/282 loss=0.2411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-623/131612518.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  >> Epoch 1: train_loss=1.1270  val_loss=0.3700\n",
            "  ** Saved best (val_loss=0.3700)\n",
            "  Ep2 step0/282 loss=0.2821\n",
            "  Ep2 step30/282 loss=0.4403\n",
            "  Ep2 step60/282 loss=0.3597\n",
            "  Ep2 step90/282 loss=0.3354\n",
            "  Ep2 step120/282 loss=0.4480\n",
            "  Ep2 step150/282 loss=0.2859\n",
            "  Ep2 step180/282 loss=0.5128\n",
            "  Ep2 step210/282 loss=0.4500\n",
            "  Ep2 step240/282 loss=0.2860\n",
            "  Ep2 step270/282 loss=0.3855\n",
            "  >> Epoch 2: train_loss=0.3240  val_loss=0.3119\n",
            "  ** Saved best (val_loss=0.3119)\n",
            "  Ep3 step0/282 loss=0.2921\n",
            "  Ep3 step30/282 loss=0.1900\n",
            "  Ep3 step60/282 loss=0.2446\n",
            "  Ep3 step90/282 loss=0.1511\n",
            "  Ep3 step120/282 loss=0.2205\n",
            "  Ep3 step150/282 loss=0.3476\n",
            "  Ep3 step180/282 loss=0.3124\n",
            "  Ep3 step210/282 loss=0.1990\n",
            "  Ep3 step240/282 loss=0.2939\n",
            "  Ep3 step270/282 loss=0.1754\n",
            "  >> Epoch 3: train_loss=0.2627  val_loss=0.3006\n",
            "  ** Saved best (val_loss=0.3006)\n",
            "  Ep4 step0/282 loss=0.2579\n",
            "  Ep4 step30/282 loss=0.2285\n",
            "  Ep4 step60/282 loss=0.1964\n",
            "  Ep4 step90/282 loss=0.1063\n",
            "  Ep4 step120/282 loss=0.3900\n",
            "  Ep4 step150/282 loss=0.1601\n",
            "  Ep4 step180/282 loss=0.2916\n",
            "  Ep4 step210/282 loss=0.2748\n",
            "  Ep4 step240/282 loss=0.1780\n",
            "  Ep4 step270/282 loss=0.1818\n",
            "  >> Epoch 4: train_loss=0.2309  val_loss=0.2972\n",
            "  ** Saved best (val_loss=0.2972)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWa8RerUCiFN"
      },
      "source": [
        "## CELL 10 — Run K-Fold Training\n",
        "Adjust N_FOLDS and NUM_EPOCHS based on time available.\n",
        "3 folds × 3 epochs ≈ 1.5–2 hours on T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aa33b2d7367b44b78fca691d5ffab113",
            "bcee98aa5cd14628a1c3f9e981a47654",
            "89d64371dcba4904a4088047a99ef88b",
            "c7273c3c4f304ead888d0a926bf10e32",
            "f0a1f5006b8e493dabe93b134b5b189c",
            "f2572d555837491abd5622d48a6922ca",
            "28f4ba281170472a9fcf0ff3f78950f9",
            "f830fe523a9d4b4a8129841b108a08dd",
            "7f5b6edfc53a492db7c87399d2bdf431",
            "16fd4d7cb09b4cde9991371d34b79b92",
            "9a58c3c1d0304333a45ca95bfeb62baf",
            "8c6df1358dfd479b91e8914772d7be22",
            "e1181e729eb34c36bc6502aecd89976d",
            "168ea950f6694f8c8608a38cf8894155",
            "6a000f5e72a24af5af5f61fbd1dbc3b9",
            "e94c2b6244eb4da8adc44cfcc0e0ab2b",
            "5706873736074723b535a66f89312478",
            "90d5b91c1503470eb177e1575bebe79d",
            "34dcfbc0a8bc429babba1fc5f41d2828",
            "d898277b229a407c8427adcef9946d68",
            "9751b8ab6b3f45dcbbe31287d848635c",
            "717fd59258144a1ea159aed117072ed5",
            "6aad1166bcdf4d359ee9c1e821d6c860",
            "e4f7d43545d4426ba4f0588c22505617",
            "591b171fe4944794a3d037cbb90de23f",
            "3ddee91a0a8c49829899d8bc97fc6b08",
            "cec6b644573d4fb2b1c792edc9941e00",
            "ed931fb03e0d4c68b4b77990f429dffe",
            "08b6bdba996e4f5f8a29c0ee1369dc05",
            "f26f6df1fa94413086018d08d53ae611",
            "46bc01d0357942a38177598c0861f436",
            "308a8fdf924441a1a742d50624c47475",
            "4c78e5af83754ce987cbadd1c8b01677"
          ]
        },
        "id": "4pYx3wsjCiFN",
        "outputId": "9f600897-14f8-4d33-a626-46362a977ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== FOLD 1 ==========\n",
            "Train: 3333 | Val: 1667\n",
            "Using device: cuda\n",
            "Building dataset from 3333 examples...\n",
            "Done.\n",
            "Building dataset from 1667 examples...\n",
            "Done.\n",
            "Train batches: 209\n",
            "Val batches: 105\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa33b2d7367b44b78fca691d5ffab113"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertForTokenClassification LOAD REPORT from: deepset/gbert-large\n",
            "Key                                        | Status     | \n",
            "-------------------------------------------+------------+-\n",
            "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
            "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
            "cls.predictions.bias                       | UNEXPECTED | \n",
            "bert.pooler.dense.bias                     | UNEXPECTED | \n",
            "cls.seq_relationship.bias                  | UNEXPECTED | \n",
            "cls.seq_relationship.weight                | UNEXPECTED | \n",
            "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
            "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
            "bert.pooler.dense.weight                   | UNEXPECTED | \n",
            "classifier.bias                            | MISSING    | \n",
            "classifier.weight                          | MISSING    | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
            "/tmp/ipython-input-623/131612518.py:48: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/tmp/ipython-input-623/131612518.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "/tmp/ipython-input-623/131612518.py:71: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  scheduler.step()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Ep1 step0/209 loss=4.0818\n",
            "  Ep1 step30/209 loss=3.2397\n",
            "  Ep1 step60/209 loss=1.0804\n",
            "  Ep1 step90/209 loss=0.8424\n",
            "  Ep1 step120/209 loss=0.6207\n",
            "  Ep1 step150/209 loss=0.5250\n",
            "  Ep1 step180/209 loss=0.3865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-623/131612518.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  >> Epoch 1: train_loss=1.2360  val_loss=0.3919\n",
            "  ** Saved best (val_loss=0.3919)\n",
            "  Ep2 step0/209 loss=0.3395\n",
            "  Ep2 step30/209 loss=0.5906\n",
            "  Ep2 step60/209 loss=0.3077\n",
            "  Ep2 step90/209 loss=0.4280\n",
            "  Ep2 step120/209 loss=0.5283\n",
            "  Ep2 step150/209 loss=0.3293\n",
            "  Ep2 step180/209 loss=0.2378\n",
            "  >> Epoch 2: train_loss=0.3360  val_loss=0.3397\n",
            "  ** Saved best (val_loss=0.3397)\n",
            "  Ep3 step0/209 loss=0.2966\n",
            "  Ep3 step30/209 loss=0.2120\n",
            "  Ep3 step60/209 loss=0.4318\n",
            "  Ep3 step90/209 loss=0.1770\n",
            "  Ep3 step120/209 loss=0.3104\n",
            "  Ep3 step150/209 loss=0.2918\n",
            "  Ep3 step180/209 loss=0.2600\n",
            "  >> Epoch 3: train_loss=0.2870  val_loss=0.3256\n",
            "  ** Saved best (val_loss=0.3256)\n",
            "  Ep4 step0/209 loss=0.3048\n",
            "  Ep4 step30/209 loss=0.3011\n",
            "  Ep4 step60/209 loss=0.2571\n",
            "  Ep4 step90/209 loss=0.2097\n",
            "  Ep4 step120/209 loss=0.2200\n",
            "  Ep4 step150/209 loss=0.2123\n",
            "  Ep4 step180/209 loss=0.2197\n",
            "  >> Epoch 4: train_loss=0.2452  val_loss=0.3201\n",
            "  ** Saved best (val_loss=0.3201)\n",
            "\n",
            "========== FOLD 2 ==========\n",
            "Train: 3333 | Val: 1667\n",
            "Using device: cuda\n",
            "Building dataset from 3333 examples...\n",
            "Done.\n",
            "Building dataset from 1667 examples...\n",
            "Done.\n",
            "Train batches: 209\n",
            "Val batches: 105\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c6df1358dfd479b91e8914772d7be22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertForTokenClassification LOAD REPORT from: deepset/gbert-large\n",
            "Key                                        | Status     | \n",
            "-------------------------------------------+------------+-\n",
            "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
            "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
            "cls.predictions.bias                       | UNEXPECTED | \n",
            "bert.pooler.dense.bias                     | UNEXPECTED | \n",
            "cls.seq_relationship.bias                  | UNEXPECTED | \n",
            "cls.seq_relationship.weight                | UNEXPECTED | \n",
            "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
            "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
            "bert.pooler.dense.weight                   | UNEXPECTED | \n",
            "classifier.bias                            | MISSING    | \n",
            "classifier.weight                          | MISSING    | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Ep1 step0/209 loss=3.8932\n",
            "  Ep1 step30/209 loss=3.1379\n",
            "  Ep1 step60/209 loss=1.4645\n",
            "  Ep1 step90/209 loss=1.0188\n",
            "  Ep1 step120/209 loss=0.4713\n",
            "  Ep1 step150/209 loss=0.3290\n",
            "  Ep1 step180/209 loss=0.3047\n",
            "  >> Epoch 1: train_loss=1.2814  val_loss=0.3744\n",
            "  ** Saved best (val_loss=0.3744)\n",
            "  Ep2 step0/209 loss=0.4708\n",
            "  Ep2 step30/209 loss=0.3702\n",
            "  Ep2 step60/209 loss=0.3086\n",
            "  Ep2 step90/209 loss=0.3121\n",
            "  Ep2 step120/209 loss=0.2706\n",
            "  Ep2 step150/209 loss=0.3121\n",
            "  Ep2 step180/209 loss=0.3234\n",
            "  >> Epoch 2: train_loss=0.3411  val_loss=0.2991\n",
            "  ** Saved best (val_loss=0.2991)\n",
            "  Ep3 step0/209 loss=0.3489\n",
            "  Ep3 step30/209 loss=0.2166\n",
            "  Ep3 step60/209 loss=0.1942\n",
            "  Ep3 step90/209 loss=0.4346\n",
            "  Ep3 step120/209 loss=0.3529\n",
            "  Ep3 step150/209 loss=0.3030\n",
            "  Ep3 step180/209 loss=0.2432\n",
            "  >> Epoch 3: train_loss=0.2782  val_loss=0.2834\n",
            "  ** Saved best (val_loss=0.2834)\n",
            "  Ep4 step0/209 loss=0.2942\n",
            "  Ep4 step30/209 loss=0.3282\n",
            "  Ep4 step60/209 loss=0.2628\n",
            "  Ep4 step90/209 loss=0.1960\n",
            "  Ep4 step120/209 loss=0.1574\n",
            "  Ep4 step150/209 loss=0.3205\n",
            "  Ep4 step180/209 loss=0.2870\n",
            "  >> Epoch 4: train_loss=0.2448  val_loss=0.2827\n",
            "  ** Saved best (val_loss=0.2827)\n",
            "\n",
            "========== FOLD 3 ==========\n",
            "Train: 3334 | Val: 1666\n",
            "Using device: cuda\n",
            "Building dataset from 3334 examples...\n",
            "Done.\n",
            "Building dataset from 1666 examples...\n",
            "Done.\n",
            "Train batches: 209\n",
            "Val batches: 105\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aad1166bcdf4d359ee9c1e821d6c860"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertForTokenClassification LOAD REPORT from: deepset/gbert-large\n",
            "Key                                        | Status     | \n",
            "-------------------------------------------+------------+-\n",
            "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
            "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
            "cls.predictions.bias                       | UNEXPECTED | \n",
            "bert.pooler.dense.bias                     | UNEXPECTED | \n",
            "cls.seq_relationship.bias                  | UNEXPECTED | \n",
            "cls.seq_relationship.weight                | UNEXPECTED | \n",
            "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
            "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
            "bert.pooler.dense.weight                   | UNEXPECTED | \n",
            "classifier.bias                            | MISSING    | \n",
            "classifier.weight                          | MISSING    | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Ep1 step0/209 loss=3.8803\n",
            "  Ep1 step30/209 loss=3.0854\n",
            "  Ep1 step60/209 loss=1.2511\n",
            "  Ep1 step90/209 loss=0.8752\n",
            "  Ep1 step120/209 loss=0.6724\n",
            "  Ep1 step150/209 loss=0.5506\n",
            "  Ep1 step180/209 loss=0.5680\n",
            "  >> Epoch 1: train_loss=1.2651  val_loss=0.3895\n",
            "  ** Saved best (val_loss=0.3895)\n",
            "  Ep2 step0/209 loss=0.4154\n",
            "  Ep2 step30/209 loss=0.2989\n",
            "  Ep2 step60/209 loss=0.2804\n",
            "  Ep2 step90/209 loss=0.2589\n",
            "  Ep2 step120/209 loss=0.3835\n",
            "  Ep2 step150/209 loss=0.2827\n",
            "  Ep2 step180/209 loss=0.4234\n",
            "  >> Epoch 2: train_loss=0.3673  val_loss=0.4165\n",
            "  Ep3 step0/209 loss=0.5709\n",
            "  Ep3 step30/209 loss=0.4306\n",
            "  Ep3 step60/209 loss=0.4465\n",
            "  Ep3 step90/209 loss=0.3658\n",
            "  Ep3 step120/209 loss=0.3278\n",
            "  Ep3 step150/209 loss=0.3852\n",
            "  Ep3 step180/209 loss=0.3470\n",
            "  >> Epoch 3: train_loss=0.3352  val_loss=0.3618\n",
            "  ** Saved best (val_loss=0.3618)\n",
            "  Ep4 step0/209 loss=0.2793\n",
            "  Ep4 step30/209 loss=0.1991\n",
            "  Ep4 step60/209 loss=0.4304\n",
            "  Ep4 step90/209 loss=0.3635\n",
            "  Ep4 step120/209 loss=0.3820\n",
            "  Ep4 step150/209 loss=0.3168\n",
            "  Ep4 step180/209 loss=0.2919\n",
            "  >> Epoch 4: train_loss=0.3638  val_loss=0.4446\n",
            "\n",
            "All 3 folds complete!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# ================================================================\n",
        "# TIME vs QUALITY:\n",
        "# N_FOLDS=3, NUM_EPOCHS=4  → best, ~2-3 hours\n",
        "# N_FOLDS=3, NUM_EPOCHS=3  → good, ~1.5 hours\n",
        "# N_FOLDS=1, NUM_EPOCHS=4  → fast, single model, ~45 min\n",
        "# BATCH_SIZE: reduce to 8 if CUDA OOM\n",
        "# ================================================================\n",
        "N_FOLDS    = 3\n",
        "NUM_EPOCHS = 4\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "fold_models = []\n",
        "fold_val_indices = []  # save for threshold tuning\n",
        "\n",
        "for fold_num, (tr_idx, va_idx) in enumerate(kf.split(train_examples), 1):\n",
        "    fold_val_indices.append(va_idx)\n",
        "    tr_exs = [train_examples[i] for i in tr_idx]\n",
        "    va_exs = [train_examples[i] for i in va_idx]\n",
        "\n",
        "    model = train_fold(tr_exs, va_exs,\n",
        "                       fold_num=fold_num,\n",
        "                       num_epochs=NUM_EPOCHS,\n",
        "                       batch_size=BATCH_SIZE)\n",
        "    fold_models.append(model)\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f'\\nAll {N_FOLDS} folds complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9oI9WCvCiFN"
      },
      "source": [
        "## CELL 11 — Inference Helper Functions\n",
        "These handle: ensemble logit averaging, threshold, subword→word mapping, span extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "274CkfjQCiFN",
        "outputId": "826a4827-23f4-4a13-9d01-6ebdd7e67a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference helpers ready.\n"
          ]
        }
      ],
      "source": [
        "def get_word_level_predictions(models, tokens, threshold):\n",
        "    \"\"\"\n",
        "    Given a list of word tokens, runs ensemble inference and returns\n",
        "    a word-level list of predicted BIO tags.\n",
        "    \"\"\"\n",
        "    enc = tokenizer(\n",
        "        tokens,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    word_ids = enc.word_ids(batch_index=0)\n",
        "\n",
        "    input_ids = enc['input_ids'].to(DEVICE)\n",
        "    attn_mask = enc['attention_mask'].to(DEVICE)\n",
        "\n",
        "    # Ensemble: average logits across all fold models\n",
        "    all_logits = []\n",
        "    for m in models:\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = m(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "        all_logits.append(logits.cpu().float())\n",
        "    avg_logits = torch.stack(all_logits).mean(0).squeeze(0)  # (seq_len, num_labels)\n",
        "\n",
        "    probs      = torch.softmax(avg_logits, dim=-1)\n",
        "    max_probs, pred_ids = probs.max(dim=-1)\n",
        "\n",
        "    # Map subtokens → words (first subtoken only)\n",
        "    word_preds = {}  # word_idx → predicted label\n",
        "    for pos, wid in enumerate(word_ids):\n",
        "        if wid is None or wid in word_preds:\n",
        "            continue\n",
        "        label = id2label[pred_ids[pos].item()]\n",
        "        conf  = max_probs[pos].item()\n",
        "        word_preds[wid] = label if (label != 'O' and conf >= threshold) else 'O'\n",
        "\n",
        "    return [word_preds.get(i, 'O') for i in range(len(tokens))]\n",
        "\n",
        "\n",
        "def bio_to_spans(tokens, bio_tags):\n",
        "    \"\"\"\n",
        "    Converts BIO tag list to list of (entity_type, aspect_value) tuples.\n",
        "    Multi-token spans are joined with single whitespace (ASCII 32).\n",
        "    \"\"\"\n",
        "    spans = []\n",
        "    i = 0\n",
        "    while i < len(bio_tags):\n",
        "        tag = bio_tags[i]\n",
        "        if tag.startswith('B-'):\n",
        "            etype = tag[2:]\n",
        "            span_tokens = [tokens[i]]\n",
        "            j = i + 1\n",
        "            while j < len(bio_tags) and bio_tags[j] == f'I-{etype}':\n",
        "                span_tokens.append(tokens[j])\n",
        "                j += 1\n",
        "            # Join with single ASCII space (competition requirement)\n",
        "            aspect_value = ' '.join(span_tokens)\n",
        "            spans.append((etype, aspect_value))\n",
        "            i = j\n",
        "        else:\n",
        "            i += 1\n",
        "    return spans\n",
        "\n",
        "\n",
        "print('Inference helpers ready.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#claude ek mein sab\n",
        "# ================================================================\n",
        "# COMPLETE FINAL SUBMISSION CELL\n",
        "# Run this after Cell 11 (inference helpers)\n",
        "# Does: per-entity threshold tuning + category filtering + submission\n",
        "# ================================================================\n",
        "\n",
        "import csv as csv_mod\n",
        "import datetime\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 1: CATEGORY-ASPECT RULES (from Annexure)\n",
        "# ----------------------------------------------------------------\n",
        "VALID_ASPECTS = {\n",
        "    '1': {  # Car Brake Component Kits\n",
        "        'Anzahl_Der_Einheiten', 'Besonderheiten', 'Breite',\n",
        "        'Bremsscheiben-Aussendurchmesser', 'Bremsscheibenart',\n",
        "        'Einbauposition', 'Farbe', 'Größe', 'Hersteller',\n",
        "        'Herstellernummer', 'Herstellungsland_Und_-Region',\n",
        "        'Im_Lieferumfang_Enthalten', 'Kompatible_Fahrzeug_Marke',\n",
        "        'Kompatibles_Fahrzeug_Jahr', 'Kompatibles_Fahrzeug_Modell',\n",
        "        'Material', 'Maßeinheit', 'Modell', 'O',\n",
        "        'Oberflächenbeschaffenheit', 'Oe/Oem_Referenznummer(N)',\n",
        "        'Produktart', 'Produktlinie', 'Stärke', 'Technologie',\n",
        "    },\n",
        "    '2': {  # Car Engine Timing Kits\n",
        "        'Anwendung', 'Anzahl_Der_Einheiten', 'Besonderheiten',\n",
        "        'Breite', 'Einbauposition', 'Größe', 'Hersteller',\n",
        "        'Herstellernummer', 'Im_Lieferumfang_Enthalten',\n",
        "        'Kompatible_Fahrzeug_Marke', 'Kompatibles_Fahrzeug_Jahr',\n",
        "        'Kompatibles_Fahrzeug_Modell', 'Länge', 'Maßeinheit',\n",
        "        'Menge', 'Modell', 'O', 'Oe/Oem_Referenznummer(N)',\n",
        "        'Produktart', 'SAE_Viskosität', 'Zähnezahl',\n",
        "    }\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 2: UPDATED INFERENCE WITH FIXED AUTOCAST WARNING\n",
        "# ----------------------------------------------------------------\n",
        "def get_word_level_predictions_per_entity(models, tokens, entity_thresholds, default_threshold=0.5):\n",
        "    enc = tokenizer(\n",
        "        tokens,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    word_ids = enc.word_ids(batch_index=0)\n",
        "    input_ids = enc['input_ids'].to(DEVICE)\n",
        "    attn_mask  = enc['attention_mask'].to(DEVICE)\n",
        "\n",
        "    all_logits = []\n",
        "    for m in models:\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast('cuda'):  # fixed warning\n",
        "                logits = m(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "        all_logits.append(logits.cpu().float())\n",
        "    avg_logits = torch.stack(all_logits).mean(0).squeeze(0)\n",
        "\n",
        "    probs = torch.softmax(avg_logits, dim=-1)\n",
        "    max_probs, pred_ids = probs.max(dim=-1)\n",
        "\n",
        "    word_preds = {}\n",
        "    for pos, wid in enumerate(word_ids):\n",
        "        if wid is None or wid in word_preds:\n",
        "            continue\n",
        "        label = id2label[pred_ids[pos].item()]\n",
        "        conf  = max_probs[pos].item()\n",
        "        if label == 'O':\n",
        "            word_preds[wid] = 'O'\n",
        "        else:\n",
        "            etype = label.replace('B-', '').replace('I-', '')\n",
        "            threshold = entity_thresholds.get(etype, default_threshold)\n",
        "            word_preds[wid] = label if conf >= threshold else 'O'\n",
        "\n",
        "    return [word_preds.get(i, 'O') for i in range(len(tokens))]\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 3: F-BETA SCORER\n",
        "# ----------------------------------------------------------------\n",
        "def compute_fbeta_score(pred_spans_list, gold_spans_list, beta=0.2):\n",
        "    tp = fp = fn = 0\n",
        "    for preds, golds in zip(pred_spans_list, gold_spans_list):\n",
        "        pred_counts = Counter(preds)\n",
        "        gold_counts = Counter(golds)\n",
        "        for span, count in pred_counts.items():\n",
        "            matched = min(count, gold_counts.get(span, 0))\n",
        "            tp += matched\n",
        "            fp += count - matched\n",
        "        for span, count in gold_counts.items():\n",
        "            matched = min(count, pred_counts.get(span, 0))\n",
        "            fn += count - matched\n",
        "    precision = tp / (tp + fp + 1e-9)\n",
        "    recall    = tp / (tp + fn + 1e-9)\n",
        "    fbeta = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-9)\n",
        "    return precision, recall, fbeta\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 4: GLOBAL THRESHOLD TUNING\n",
        "# ----------------------------------------------------------------\n",
        "print('='*60)\n",
        "print('STEP 4: Global threshold tuning...')\n",
        "print('='*60)\n",
        "\n",
        "# Use last fold's validation set\n",
        "tune_examples = val_exs\n",
        "print(f'Tuning on {len(tune_examples)} validation examples...')\n",
        "\n",
        "# Gold spans (excluding O)\n",
        "gold_spans_all = []\n",
        "for ex in tune_examples:\n",
        "    spans = bio_to_spans(ex['tokens'], ex['tags'])\n",
        "    gold_spans_all.append([(t, v) for t, v in spans if t != 'O'])\n",
        "\n",
        "print(f'Tuning on {len(tune_examples)} validation examples')\n",
        "print(f'\\n{\"Threshold\":>10} | {\"Precision\":>10} | {\"Recall\":>8} | {\"F-beta(0.2)\":>12}')\n",
        "print('-' * 50)\n",
        "\n",
        "best_threshold = 0.5\n",
        "best_fbeta     = 0.0\n",
        "\n",
        "for threshold in np.arange(0.40, 0.97, 0.03):\n",
        "    # Use simple uniform threshold for global tuning\n",
        "    simple_thresholds = {\n",
        "        label.replace('B-', ''): threshold\n",
        "        for label in label2id if label.startswith('B-')\n",
        "    }\n",
        "    pred_spans_all = []\n",
        "    for ex in tune_examples:\n",
        "        bio_preds = get_word_level_predictions_per_entity(\n",
        "            fold_models, ex['tokens'], simple_thresholds, threshold)\n",
        "        spans = bio_to_spans(ex['tokens'], bio_preds)\n",
        "        pred_spans_all.append([(t, v) for t, v in spans if t != 'O'])\n",
        "\n",
        "    p, r, f = compute_fbeta_score(pred_spans_all, gold_spans_all)\n",
        "    marker = ' <-- BEST' if f > best_fbeta else ''\n",
        "    print(f'{threshold:>10.2f} | {p:>10.4f} | {r:>8.4f} | {f:>12.4f}{marker}')\n",
        "\n",
        "    if f > best_fbeta:\n",
        "        best_fbeta     = f\n",
        "        best_threshold = threshold\n",
        "\n",
        "print(f'\\nBest global threshold: {best_threshold:.2f}  (F-beta={best_fbeta:.4f})')\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 5: PER-ENTITY THRESHOLD TUNING\n",
        "# ----------------------------------------------------------------\n",
        "print('\\n' + '='*60)\n",
        "print('STEP 5: Per-entity threshold tuning...')\n",
        "print('='*60)\n",
        "\n",
        "# Entities with lots of FPs from your error analysis → push threshold UP\n",
        "HIGH_FP_ENTITIES = [\n",
        "    'Kompatibles_Fahrzeug_Modell',\n",
        "    'Einbauposition',\n",
        "    'Produktart',\n",
        "    'Im_Lieferumfang_Enthalten'\n",
        "]\n",
        "\n",
        "# Start every entity at global best\n",
        "tuned_thresholds = {\n",
        "    label.replace('B-', ''): best_threshold\n",
        "    for label in label2id if label.startswith('B-')\n",
        "}\n",
        "\n",
        "for entity in HIGH_FP_ENTITIES:\n",
        "    if f'B-{entity}' not in label2id:\n",
        "        print(f'  Skipping {entity} (not in label set)')\n",
        "        continue\n",
        "\n",
        "    best_e_t     = best_threshold\n",
        "    best_e_fbeta = 0.0\n",
        "\n",
        "    for t in np.arange(best_threshold, 0.97, 0.02):\n",
        "        test_thresholds = dict(tuned_thresholds)\n",
        "        test_thresholds[entity] = t\n",
        "\n",
        "        pred_spans_all   = []\n",
        "        gold_spans_local = []\n",
        "        for ex in tune_examples:\n",
        "            bio_preds = get_word_level_predictions_per_entity(\n",
        "                fold_models, ex['tokens'], test_thresholds)\n",
        "            spans = bio_to_spans(ex['tokens'], bio_preds)\n",
        "            pred_spans_all.append([(t2, v) for t2, v in spans if t2 == entity])\n",
        "            gold_spans = bio_to_spans(ex['tokens'], ex['tags'])\n",
        "            gold_spans_local.append([(t2, v) for t2, v in gold_spans if t2 == entity])\n",
        "\n",
        "        p, r, f = compute_fbeta_score(pred_spans_all, gold_spans_local)\n",
        "        if f > best_e_fbeta:\n",
        "            best_e_fbeta = f\n",
        "            best_e_t     = t\n",
        "\n",
        "    tuned_thresholds[entity] = best_e_t\n",
        "    print(f'  {entity}: {best_threshold:.2f} → {best_e_t:.2f}  (fbeta={best_e_fbeta:.4f})')\n",
        "\n",
        "print('\\nAll tuned thresholds:')\n",
        "for entity, t in sorted(tuned_thresholds.items()):\n",
        "    changed = ' *' if abs(t - best_threshold) > 0.01 else ''\n",
        "    print(f'  {entity}: {t:.2f}{changed}')\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 6: GENERATE SUBMISSION WITH CATEGORY FILTERING\n",
        "# ----------------------------------------------------------------\n",
        "print('\\n' + '='*60)\n",
        "print('STEP 6: Generating submission...')\n",
        "print('='*60)\n",
        "\n",
        "rows    = []\n",
        "skipped = 0\n",
        "\n",
        "for i, ex in enumerate(quiz_examples):\n",
        "    if i % 3000 == 0:\n",
        "        print(f'  Processing {i}/{len(quiz_examples)}...')\n",
        "\n",
        "    category              = str(ex['category_id'])\n",
        "    valid_for_category    = VALID_ASPECTS.get(category, set())\n",
        "\n",
        "    bio_preds = get_word_level_predictions_per_entity(\n",
        "        fold_models, ex['tokens'], tuned_thresholds)\n",
        "    spans = bio_to_spans(ex['tokens'], bio_preds)\n",
        "\n",
        "    for entity_type, aspect_value in spans:\n",
        "        if entity_type == 'O':\n",
        "            continue\n",
        "        if entity_type not in valid_for_category:\n",
        "            skipped += 1\n",
        "            continue\n",
        "        rows.append({\n",
        "            'record_number': ex['record_id'],\n",
        "            'category_id':   ex['category_id'],\n",
        "            'aspect_name':   entity_type,\n",
        "            'aspect_value':  aspect_value\n",
        "        })\n",
        "\n",
        "df_sub = pd.DataFrame(rows)\n",
        "print(f'\\nSkipped {skipped} predictions (wrong category)')\n",
        "print(f'Total submission rows: {len(df_sub)}')\n",
        "print(f'Records with predictions: {df_sub[\"record_number\"].nunique()} / {len(quiz_examples)}')\n",
        "print('\\nAspect distribution:')\n",
        "print(df_sub['aspect_name'].value_counts().to_string())\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 7: VALIDATE & SAVE\n",
        "# ----------------------------------------------------------------\n",
        "print('\\n' + '='*60)\n",
        "print('STEP 7: Validating & saving...')\n",
        "print('='*60)\n",
        "\n",
        "# Validation checks\n",
        "assert df_sub.isnull().sum().sum() == 0, 'NULL values found!'\n",
        "assert not df_sub['aspect_value'].str.contains('\\t').any(), 'TAB in aspect values!'\n",
        "\n",
        "# Check no invalid category-aspect pairs\n",
        "for _, row in df_sub.iterrows():\n",
        "    cat    = str(row['category_id'])\n",
        "    aspect = row['aspect_name']\n",
        "    assert aspect in VALID_ASPECTS.get(cat, set()), \\\n",
        "        f'Invalid aspect {aspect} for category {cat}!'\n",
        "\n",
        "print('All validation checks passed!')\n",
        "\n",
        "ts         = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
        "sub_path   = f'{OUTPUT_DIR}/submission_final_{ts}.tsv'\n",
        "\n",
        "df_sub.to_csv(\n",
        "    sub_path,\n",
        "    sep='\\t',\n",
        "    index=False,\n",
        "    quoting=csv_mod.QUOTE_NONE,\n",
        "    encoding='utf-8'\n",
        ")\n",
        "print(f'Saved to: {sub_path}')\n",
        "\n",
        "# Download to your computer\n",
        "from google.colab import files\n",
        "files.download(sub_path)\n",
        "print('\\nDone! Submit the downloaded .tsv file to EvalAI.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KG4QGMWgKOsd",
        "outputId": "48d85eef-4cfa-46be-8b6c-90a721ad2987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 4: Global threshold tuning...\n",
            "============================================================\n",
            "Tuning on 500 validation examples...\n",
            "Tuning on 500 validation examples\n",
            "\n",
            " Threshold |  Precision |   Recall |  F-beta(0.2)\n",
            "--------------------------------------------------\n",
            "      0.40 |     0.8964 |   0.8820 |       0.8958 <-- BEST\n",
            "      0.43 |     0.8968 |   0.8815 |       0.8962 <-- BEST\n",
            "      0.46 |     0.8986 |   0.8802 |       0.8979 <-- BEST\n",
            "      0.49 |     0.8990 |   0.8794 |       0.8983 <-- BEST\n",
            "      0.52 |     0.9009 |   0.8762 |       0.9000 <-- BEST\n",
            "      0.55 |     0.9048 |   0.8724 |       0.9035 <-- BEST\n",
            "      0.58 |     0.9072 |   0.8690 |       0.9057 <-- BEST\n",
            "      0.61 |     0.9089 |   0.8652 |       0.9071 <-- BEST\n",
            "      0.64 |     0.9144 |   0.8612 |       0.9123 <-- BEST\n",
            "      0.67 |     0.9182 |   0.8564 |       0.9156 <-- BEST\n",
            "      0.70 |     0.9194 |   0.8521 |       0.9166 <-- BEST\n",
            "      0.73 |     0.9197 |   0.8468 |       0.9167 <-- BEST\n",
            "      0.76 |     0.9204 |   0.8420 |       0.9171 <-- BEST\n",
            "      0.79 |     0.9204 |   0.8361 |       0.9168\n",
            "      0.82 |     0.9163 |   0.8273 |       0.9126\n",
            "      0.85 |     0.9159 |   0.8199 |       0.9118\n",
            "      0.88 |     0.9098 |   0.8073 |       0.9054\n",
            "      0.91 |     0.9032 |   0.7918 |       0.8983\n",
            "      0.94 |     0.8860 |   0.7590 |       0.8803\n",
            "\n",
            "Best global threshold: 0.76  (F-beta=0.9171)\n",
            "\n",
            "============================================================\n",
            "STEP 5: Per-entity threshold tuning...\n",
            "============================================================\n",
            "  Kompatibles_Fahrzeug_Modell: 0.76 → 0.76  (fbeta=0.7709)\n",
            "  Einbauposition: 0.76 → 0.78  (fbeta=0.9508)\n",
            "  Produktart: 0.76 → 0.78  (fbeta=0.9103)\n",
            "  Im_Lieferumfang_Enthalten: 0.76 → 0.96  (fbeta=0.9668)\n",
            "\n",
            "All tuned thresholds:\n",
            "  Anwendung: 0.76\n",
            "  Anzahl_Der_Einheiten: 0.76\n",
            "  Besonderheiten: 0.76\n",
            "  Breite: 0.76\n",
            "  Bremsscheiben-Aussendurchmesser: 0.76\n",
            "  Bremsscheibenart: 0.76\n",
            "  Einbauposition: 0.78 *\n",
            "  Farbe: 0.76\n",
            "  Größe: 0.76\n",
            "  Hersteller: 0.76\n",
            "  Herstellernummer: 0.76\n",
            "  Herstellungsland_Und_-Region: 0.76\n",
            "  Im_Lieferumfang_Enthalten: 0.96 *\n",
            "  Kompatible_Fahrzeug_Marke: 0.76\n",
            "  Kompatibles_Fahrzeug_Jahr: 0.76\n",
            "  Kompatibles_Fahrzeug_Modell: 0.76\n",
            "  Länge: 0.76\n",
            "  Material: 0.76\n",
            "  Maßeinheit: 0.76\n",
            "  Menge: 0.76\n",
            "  Modell: 0.76\n",
            "  Oberflächenbeschaffenheit: 0.76\n",
            "  Oe/Oem_Referenznummer(N): 0.76\n",
            "  Produktart: 0.78 *\n",
            "  Produktlinie: 0.76\n",
            "  SAE_Viskosität: 0.76\n",
            "  Stärke: 0.76\n",
            "  Technologie: 0.76\n",
            "  Zähnezahl: 0.76\n",
            "\n",
            "============================================================\n",
            "STEP 6: Generating submission...\n",
            "============================================================\n",
            "  Processing 0/25000...\n",
            "  Processing 3000/25000...\n",
            "  Processing 6000/25000...\n",
            "  Processing 9000/25000...\n",
            "  Processing 12000/25000...\n",
            "  Processing 15000/25000...\n",
            "  Processing 18000/25000...\n",
            "  Processing 21000/25000...\n",
            "  Processing 24000/25000...\n",
            "\n",
            "Skipped 223 predictions (wrong category)\n",
            "Total submission rows: 160700\n",
            "Records with predictions: 24910 / 25000\n",
            "\n",
            "Aspect distribution:\n",
            "aspect_name\n",
            "Kompatibles_Fahrzeug_Modell        35509\n",
            "Kompatible_Fahrzeug_Marke          28436\n",
            "Im_Lieferumfang_Enthalten          21151\n",
            "Hersteller                         21030\n",
            "Produktart                         16588\n",
            "Herstellernummer                   13839\n",
            "Einbauposition                     11592\n",
            "Bremsscheiben-Aussendurchmesser     4077\n",
            "Anzahl_Der_Einheiten                3716\n",
            "Kompatibles_Fahrzeug_Jahr           2772\n",
            "Bremsscheibenart                    1311\n",
            "Maßeinheit                           534\n",
            "Produktlinie                         123\n",
            "Modell                                22\n",
            "\n",
            "============================================================\n",
            "STEP 7: Validating & saving...\n",
            "============================================================\n",
            "All validation checks passed!\n",
            "Saved to: /content/drive/MyDrive/ebay_ner/outputs/submission_final_20260226_1937.tsv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_27b26dfa-1be2-44c1-ba51-a87beb996148\", \"submission_final_20260226_1937.tsv\", 6028500)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done! Submit the downloaded .tsv file to EvalAI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# COMPLETE FINAL SUBMISSION CELL - CORRECTED\n",
        "# ================================================================\n",
        "\n",
        "import csv as csv_mod\n",
        "import datetime\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# VALID ASPECTS PER CATEGORY (definitive from Annexure)\n",
        "# ----------------------------------------------------------------\n",
        "VALID_ASPECTS = {\n",
        "    '1': {\n",
        "        'Anzahl_Der_Einheiten', 'Besonderheiten',\n",
        "        'Bremsscheiben-Aussendurchmesser', 'Bremsscheibenart',\n",
        "        'Einbauposition', 'Farbe', 'Größe', 'Hersteller',\n",
        "        'Herstellernummer', 'Herstellungsland_Und_-Region',\n",
        "        'Im_Lieferumfang_Enthalten', 'Kompatible_Fahrzeug_Marke',\n",
        "        'Kompatibles_Fahrzeug_Jahr', 'Kompatibles_Fahrzeug_Modell',\n",
        "        'Material', 'Maßeinheit', 'Modell', 'O',\n",
        "        'Oberflächenbeschaffenheit', 'Oe/Oem_Referenznummer(N)',\n",
        "        'Produktart', 'Produktlinie', 'Stärke', 'Technologie',\n",
        "    },\n",
        "    '2': {\n",
        "        'Anwendung', 'Anzahl_Der_Einheiten', 'Besonderheiten',\n",
        "        'Breite', 'Einbauposition', 'Größe', 'Hersteller',\n",
        "        'Herstellernummer', 'Im_Lieferumfang_Enthalten',\n",
        "        'Kompatible_Fahrzeug_Marke', 'Kompatibles_Fahrzeug_Jahr',\n",
        "        'Kompatibles_Fahrzeug_Modell', 'Länge', 'Maßeinheit',\n",
        "        'Menge', 'Modell', 'O', 'Oe/Oem_Referenznummer(N)',\n",
        "        'Produktart', 'SAE_Viskosität', 'Zähnezahl',\n",
        "    }\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# INFERENCE FUNCTION (fixed autocast warning)\n",
        "# ----------------------------------------------------------------\n",
        "def get_word_level_predictions_per_entity(models, tokens, entity_thresholds,\n",
        "                                           default_threshold=0.5):\n",
        "    enc = tokenizer(\n",
        "        tokens,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    word_ids  = enc.word_ids(batch_index=0)\n",
        "    input_ids = enc['input_ids'].to(DEVICE)\n",
        "    attn_mask = enc['attention_mask'].to(DEVICE)\n",
        "\n",
        "    all_logits = []\n",
        "    for m in models:\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                logits = m(input_ids=input_ids,\n",
        "                           attention_mask=attn_mask).logits\n",
        "        all_logits.append(logits.cpu().float())\n",
        "    avg_logits = torch.stack(all_logits).mean(0).squeeze(0)\n",
        "\n",
        "    probs              = torch.softmax(avg_logits, dim=-1)\n",
        "    max_probs, pred_ids = probs.max(dim=-1)\n",
        "\n",
        "    word_preds = {}\n",
        "    for pos, wid in enumerate(word_ids):\n",
        "        if wid is None or wid in word_preds:\n",
        "            continue\n",
        "        label = id2label[pred_ids[pos].item()]\n",
        "        conf  = max_probs[pos].item()\n",
        "        if label == 'O':\n",
        "            word_preds[wid] = 'O'\n",
        "        else:\n",
        "            etype     = label.replace('B-', '').replace('I-', '')\n",
        "            threshold = entity_thresholds.get(etype, default_threshold)\n",
        "            word_preds[wid] = label if conf >= threshold else 'O'\n",
        "\n",
        "    return [word_preds.get(i, 'O') for i in range(len(tokens))]\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# F-BETA SCORER\n",
        "# ----------------------------------------------------------------\n",
        "def compute_fbeta_score(pred_spans_list, gold_spans_list, beta=0.2):\n",
        "    tp = fp = fn = 0\n",
        "    for preds, golds in zip(pred_spans_list, gold_spans_list):\n",
        "        pred_counts = Counter(preds)\n",
        "        gold_counts = Counter(golds)\n",
        "        for span, count in pred_counts.items():\n",
        "            matched  = min(count, gold_counts.get(span, 0))\n",
        "            tp      += matched\n",
        "            fp      += count - matched\n",
        "        for span, count in gold_counts.items():\n",
        "            matched  = min(count, pred_counts.get(span, 0))\n",
        "            fn      += count - matched\n",
        "    precision = tp / (tp + fp + 1e-9)\n",
        "    recall    = tp / (tp + fn + 1e-9)\n",
        "    fbeta     = ((1 + beta**2) * precision * recall /\n",
        "                 (beta**2 * precision + recall + 1e-9))\n",
        "    return precision, recall, fbeta\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 1: GLOBAL THRESHOLD TUNING\n",
        "# ----------------------------------------------------------------\n",
        "print('='*60)\n",
        "print('STEP 1: Global threshold tuning')\n",
        "print('='*60)\n",
        "\n",
        "tune_val_idx  = fold_val_indices[-1]\n",
        "tune_examples = [train_examples[i] for i in tune_val_idx]\n",
        "\n",
        "gold_spans_all = []\n",
        "for ex in tune_examples:\n",
        "    spans = bio_to_spans(ex['tokens'], ex['tags'])\n",
        "    gold_spans_all.append([(t, v) for t, v in spans if t != 'O'])\n",
        "\n",
        "print(f'Validation set: {len(tune_examples)} examples\\n')\n",
        "print(f'{\"Threshold\":>10} | {\"Precision\":>10} | {\"Recall\":>8} | {\"F-beta\":>10}')\n",
        "print('-'*46)\n",
        "\n",
        "best_threshold = 0.5\n",
        "best_fbeta     = 0.0\n",
        "\n",
        "for threshold in np.arange(0.40, 0.97, 0.03):\n",
        "    simple_t = {\n",
        "        label.replace('B-', ''): threshold\n",
        "        for label in label2id if label.startswith('B-')\n",
        "    }\n",
        "    pred_spans_all = []\n",
        "    for ex in tune_examples:\n",
        "        preds = get_word_level_predictions_per_entity(\n",
        "            fold_models, ex['tokens'], simple_t, threshold)\n",
        "        spans = bio_to_spans(ex['tokens'], preds)\n",
        "        pred_spans_all.append([(t, v) for t, v in spans if t != 'O'])\n",
        "\n",
        "    p, r, f = compute_fbeta_score(pred_spans_all, gold_spans_all)\n",
        "    marker  = ' <-- BEST' if f > best_fbeta else ''\n",
        "    print(f'{threshold:>10.2f} | {p:>10.4f} | {r:>8.4f} | {f:>10.4f}{marker}')\n",
        "\n",
        "    if f > best_fbeta:\n",
        "        best_fbeta     = f\n",
        "        best_threshold = threshold\n",
        "\n",
        "print(f'\\nBest global threshold: {best_threshold:.2f}')\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 2: PER-ENTITY THRESHOLD TUNING\n",
        "# ----------------------------------------------------------------\n",
        "print('\\n' + '='*60)\n",
        "print('STEP 2: Per-entity threshold tuning')\n",
        "print('='*60)\n",
        "\n",
        "# Based on your error analysis - these had most FPs\n",
        "HIGH_FP_ENTITIES = [\n",
        "    'Kompatibles_Fahrzeug_Modell',\n",
        "    'Einbauposition',\n",
        "    'Produktart',\n",
        "    'Im_Lieferumfang_Enthalten',\n",
        "]\n",
        "\n",
        "# Start every entity at global best\n",
        "tuned_thresholds = {\n",
        "    label.replace('B-', ''): best_threshold\n",
        "    for label in label2id if label.startswith('B-')\n",
        "}\n",
        "\n",
        "for entity in HIGH_FP_ENTITIES:\n",
        "    if f'B-{entity}' not in label2id:\n",
        "        print(f'  Skipping {entity} (not in training labels)')\n",
        "        continue\n",
        "\n",
        "    best_e_t     = best_threshold\n",
        "    best_e_fbeta = 0.0\n",
        "\n",
        "    for t in np.arange(best_threshold, 0.97, 0.02):\n",
        "        test_t = dict(tuned_thresholds)\n",
        "        test_t[entity] = t\n",
        "\n",
        "        pred_spans_all   = []\n",
        "        gold_spans_local = []\n",
        "        for ex in tune_examples:\n",
        "            preds = get_word_level_predictions_per_entity(\n",
        "                fold_models, ex['tokens'], test_t)\n",
        "            spans = bio_to_spans(ex['tokens'], preds)\n",
        "            pred_spans_all.append(\n",
        "                [(t2, v) for t2, v in spans if t2 == entity])\n",
        "            gold = bio_to_spans(ex['tokens'], ex['tags'])\n",
        "            gold_spans_local.append(\n",
        "                [(t2, v) for t2, v in gold if t2 == entity])\n",
        "\n",
        "        p, r, f = compute_fbeta_score(pred_spans_all, gold_spans_local)\n",
        "        if f > best_e_fbeta:\n",
        "            best_e_fbeta = f\n",
        "            best_e_t     = t\n",
        "\n",
        "    tuned_thresholds[entity] = best_e_t\n",
        "    changed = f'{best_threshold:.2f} → {best_e_t:.2f}'\n",
        "    print(f'  {entity}: {changed}  (fbeta={best_e_fbeta:.4f})')\n",
        "\n",
        "print('\\nFinal thresholds (changed only):')\n",
        "for entity, t in sorted(tuned_thresholds.items()):\n",
        "    if abs(t - best_threshold) > 0.01:\n",
        "        print(f'  {entity}: {t:.2f}')\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 3: GENERATE SUBMISSION FOR ALL 25,000 QUIZ RECORDS\n",
        "# ----------------------------------------------------------------\n",
        "print('\\n' + '='*60)\n",
        "print('STEP 3: Generating submission (records 5001-30000)')\n",
        "print('='*60)\n",
        "print(f'Total quiz records to process: {len(quiz_examples)}')\n",
        "\n",
        "rows    = []\n",
        "skipped = 0\n",
        "\n",
        "for i, ex in enumerate(quiz_examples):\n",
        "    if i % 3000 == 0:\n",
        "        print(f'  {i}/{len(quiz_examples)} processed...')\n",
        "\n",
        "    category           = str(ex['category_id'])\n",
        "    valid_for_category = VALID_ASPECTS.get(category, set())\n",
        "\n",
        "    bio_preds = get_word_level_predictions_per_entity(\n",
        "        fold_models, ex['tokens'], tuned_thresholds)\n",
        "    spans = bio_to_spans(ex['tokens'], bio_preds)\n",
        "\n",
        "    for entity_type, aspect_value in spans:\n",
        "        if entity_type == 'O':\n",
        "            continue\n",
        "        if entity_type not in valid_for_category:\n",
        "            skipped += 1\n",
        "            continue\n",
        "        rows.append({\n",
        "            'record_number': ex['record_id'],\n",
        "            'category_id':   ex['category_id'],\n",
        "            'aspect_name':   entity_type,\n",
        "            'aspect_value':  aspect_value\n",
        "        })\n",
        "\n",
        "df_sub = pd.DataFrame(rows)\n",
        "\n",
        "print(f'\\nSkipped {skipped} predictions (wrong category)')\n",
        "print(f'Total rows in submission: {len(df_sub)}')\n",
        "print(f'Records with ≥1 prediction: {df_sub[\"record_number\"].nunique()} / {len(quiz_examples)}')\n",
        "print(f'Records with 0 predictions: {len(quiz_examples) - df_sub[\"record_number\"].nunique()}')\n",
        "print('\\nAspect distribution:')\n",
        "print(df_sub['aspect_name'].value_counts().to_string())\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 4: VALIDATE\n",
        "# ----------------------------------------------------------------\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 4: FULL VALIDATION (checks every single row)\n",
        "# ----------------------------------------------------------------\n",
        "print('\\n' + '='*60)\n",
        "print('STEP 4: Full validation (every row)')\n",
        "print('='*60)\n",
        "\n",
        "# Check 1: No nulls\n",
        "assert df_sub.isnull().sum().sum() == 0, \\\n",
        "    'FAIL: NULL values found in submission!'\n",
        "print(f'PASSED: No null values ({len(df_sub)} rows checked)')\n",
        "\n",
        "# Check 2: No TAB characters in aspect values\n",
        "tab_mask = df_sub['aspect_value'].str.contains('\\t', regex=False)\n",
        "assert not tab_mask.any(), \\\n",
        "    f'FAIL: TAB in aspect_value at rows: {df_sub[tab_mask].index.tolist()[:5]}'\n",
        "print('PASSED: No TAB characters in aspect values')\n",
        "\n",
        "# Check 3: All record IDs are in valid quiz range\n",
        "valid_record_ids = set(ex['record_id'] for ex in quiz_examples)\n",
        "sub_record_ids   = set(df_sub['record_number'].astype(str))\n",
        "invalid_ids      = sub_record_ids - valid_record_ids\n",
        "assert len(invalid_ids) == 0, \\\n",
        "    f'FAIL: {len(invalid_ids)} record IDs not in quiz range! Examples: {list(invalid_ids)[:5]}'\n",
        "print(f'PASSED: All record IDs valid (range 5001-30000)')\n",
        "\n",
        "# Check 4: EVERY row has a valid aspect for its category\n",
        "# Build a fast lookup: (category, aspect) -> valid?\n",
        "invalid_rows = []\n",
        "for idx, row in df_sub.iterrows():\n",
        "    cat    = str(row['category_id'])\n",
        "    aspect = row['aspect_name']\n",
        "    if aspect not in VALID_ASPECTS.get(cat, set()):\n",
        "        invalid_rows.append({\n",
        "            'row':          idx,\n",
        "            'record':       row['record_number'],\n",
        "            'category':     cat,\n",
        "            'bad_aspect':   aspect,\n",
        "            'value':        row['aspect_value']\n",
        "        })\n",
        "\n",
        "if invalid_rows:\n",
        "    print(f'\\nFAIL: {len(invalid_rows)} rows have invalid category-aspect pairs!')\n",
        "    print('First 10 bad rows:')\n",
        "    for r in invalid_rows[:10]:\n",
        "        print(f\"  Row {r['row']}: record={r['record']} cat={r['category']} \"\n",
        "              f\"aspect={r['bad_aspect']} value={r['value']}\")\n",
        "    raise AssertionError('Fix the VALID_ASPECTS dict or category filtering before submitting!')\n",
        "else:\n",
        "    print(f'PASSED: All {len(df_sub)} rows have valid category-aspect pairs')\n",
        "\n",
        "# Check 5: aspect_value contains no leading/trailing whitespace\n",
        "stripped = df_sub['aspect_value'].str.strip()\n",
        "whitespace_issues = (stripped != df_sub['aspect_value']).sum()\n",
        "if whitespace_issues > 0:\n",
        "    print(f'WARNING: {whitespace_issues} aspect values have leading/trailing whitespace')\n",
        "    print('Auto-fixing...')\n",
        "    df_sub['aspect_value'] = stripped\n",
        "else:\n",
        "    print('PASSED: No leading/trailing whitespace in aspect values')\n",
        "\n",
        "# Check 6: No empty aspect values\n",
        "empty_mask = df_sub['aspect_value'].str.len() == 0\n",
        "assert not empty_mask.any(), \\\n",
        "    f'FAIL: {empty_mask.sum()} empty aspect values found!'\n",
        "print('PASSED: No empty aspect values')\n",
        "\n",
        "# Summary\n",
        "print(f'\\n=== VALIDATION SUMMARY ===')\n",
        "print(f'Total rows:              {len(df_sub)}')\n",
        "print(f'Unique records:          {df_sub[\"record_number\"].nunique()}')\n",
        "print(f'Records with 0 aspects:  {len(quiz_examples) - df_sub[\"record_number\"].nunique()}')\n",
        "print(f'Category 1 rows:         {(df_sub[\"category_id\"].astype(str) == \"1\").sum()}')\n",
        "print(f'Category 2 rows:         {(df_sub[\"category_id\"].astype(str) == \"2\").sum()}')\n",
        "print(f'All checks passed. Safe to submit.')\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 5: SAVE & DOWNLOAD\n",
        "# ----------------------------------------------------------------\n",
        "print('\\n' + '='*60)\n",
        "print('STEP 5: Saving submission')\n",
        "print('='*60)\n",
        "\n",
        "ts       = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
        "sub_path = f'{OUTPUT_DIR}/submission_{ts}.tsv'\n",
        "\n",
        "df_sub.to_csv(\n",
        "    sub_path,\n",
        "    sep='\\t',\n",
        "    index=False,\n",
        "    quoting=csv_mod.QUOTE_NONE,\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "print(f'Saved: {sub_path}')\n",
        "\n",
        "from google.colab import files\n",
        "files.download(sub_path)\n",
        "print('\\nDone! Upload the downloaded .tsv to EvalAI.')\n",
        "print('Filename must contain only letters, digits, underscores and end in .tsv')"
      ],
      "metadata": {
        "id": "ZdFMQF6NMg2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ik0dbg5CiFN"
      },
      "source": [
        "## CELL 12 — Threshold Tuning (Most Important Step!)\n",
        "F-beta=0.2 means precision is ~25× more important than recall.\n",
        "We find the threshold that maximises F-beta on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "tsN5aH59CiFN",
        "outputId": "d8a52f11-280c-409c-b2ec-d0a36ac9c35c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning on 500 validation examples...\n",
            "\n",
            "Threshold | Precision | Recall  | F-beta(0.2)\n",
            "----------------------------------------------------\n",
            "  0.40    | 0.8964    | 0.8820  | 0.8958 <-- BEST\n",
            "  0.43    | 0.8968    | 0.8815  | 0.8962 <-- BEST\n",
            "  0.46    | 0.8986    | 0.8802  | 0.8979 <-- BEST\n",
            "  0.49    | 0.8990    | 0.8794  | 0.8983 <-- BEST\n",
            "  0.52    | 0.9009    | 0.8762  | 0.9000 <-- BEST\n",
            "  0.55    | 0.9048    | 0.8724  | 0.9035 <-- BEST\n",
            "  0.58    | 0.9072    | 0.8690  | 0.9057 <-- BEST\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-497/1372630805.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mpred_spans_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtune_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mbio_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_level_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mspans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbio_to_spans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbio_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mpred_spans_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspans\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-497/4239980200.py\u001b[0m in \u001b[0;36mget_word_level_predictions\u001b[0;34m(models, tokens, threshold)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mall_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mavg_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (seq_len, num_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, **kwargs)\u001b[0m\n\u001b[1;32m   1336\u001b[0m             \u001b[0mLabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomputing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIndices\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \"\"\"\n\u001b[0;32m-> 1338\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1339\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m         )\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    697\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m     ) -> tuple[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             hidden_states = layer_module(\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnpack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTransformersKwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     ) -> tuple[torch.Tensor]:\n\u001b[0;32m--> 397\u001b[0;31m         self_attention_output, _ = self.attention(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     ) -> tuple[torch.Tensor]:\n\u001b[1;32m    325\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cross_attention\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         attention_output, attn_weights = self.self(\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# get all proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def compute_fbeta_score(pred_spans_list, gold_spans_list, beta=0.2):\n",
        "    \"\"\"\n",
        "    Span-level F-beta. Each span is (entity_type, aspect_value).\n",
        "    Matches competition evaluation: counts per (type, value) pair.\n",
        "    \"\"\"\n",
        "    tp = fp = fn = 0\n",
        "    for preds, golds in zip(pred_spans_list, gold_spans_list):\n",
        "        # Use multisets: duplicate HONDA entries all count\n",
        "        from collections import Counter\n",
        "        pred_counts = Counter(preds)\n",
        "        gold_counts = Counter(golds)\n",
        "        for span, count in pred_counts.items():\n",
        "            matched = min(count, gold_counts.get(span, 0))\n",
        "            tp += matched\n",
        "            fp += count - matched\n",
        "        for span, count in gold_counts.items():\n",
        "            matched = min(count, pred_counts.get(span, 0))\n",
        "            fn += count - matched\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-9)\n",
        "    recall    = tp / (tp + fn + 1e-9)\n",
        "    fbeta = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-9)\n",
        "    return precision, recall, fbeta\n",
        "\n",
        "\n",
        "# Use last fold's validation set for tuning\n",
        "tune_examples = val_exs\n",
        "\n",
        "print(f'Tuning on {len(tune_examples)} validation examples...')\n",
        "\n",
        "# Gold spans for validation set\n",
        "gold_spans_all = []\n",
        "for ex in tune_examples:\n",
        "    spans = bio_to_spans(ex['tokens'], ex['tags'])\n",
        "    # Exclude 'O' spans (not evaluated)\n",
        "    gold_spans_all.append([(t, v) for t, v in spans if t != 'O'])\n",
        "\n",
        "# Try different thresholds\n",
        "print('\\nThreshold | Precision | Recall  | F-beta(0.2)')\n",
        "print('-' * 52)\n",
        "\n",
        "best_threshold = 0.5\n",
        "best_fbeta = 0.0\n",
        "results = []\n",
        "\n",
        "for threshold in np.arange(0.40, 0.97, 0.03):\n",
        "    pred_spans_all = []\n",
        "    for ex in tune_examples:\n",
        "        bio_preds = get_word_level_predictions(fold_models, ex['tokens'], threshold)\n",
        "        spans = bio_to_spans(ex['tokens'], bio_preds)\n",
        "        pred_spans_all.append([(t, v) for t, v in spans if t != 'O'])\n",
        "\n",
        "    p, r, f = compute_fbeta_score(pred_spans_all, gold_spans_all)\n",
        "    results.append((threshold, p, r, f))\n",
        "    marker = ' <-- BEST' if f > best_fbeta else ''\n",
        "    print(f'  {threshold:.2f}    | {p:.4f}    | {r:.4f}  | {f:.4f}{marker}')\n",
        "\n",
        "    if f > best_fbeta:\n",
        "        best_fbeta = f\n",
        "        best_threshold = threshold\n",
        "\n",
        "print(f'\\n>>> BEST THRESHOLD = {best_threshold:.2f}')\n",
        "print(f'>>> F-beta(0.2) = {best_fbeta:.4f}')\n",
        "\n",
        "# Also try a slightly more aggressive threshold (often better on test due to precision weight)\n",
        "aggressive_threshold = min(best_threshold + 0.06, 0.95)\n",
        "print(f'>>> Also try aggressive threshold = {aggressive_threshold:.2f} (submit both, compare on LB)')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# CELL 12B: Per-entity threshold tuning\n",
        "# Different entity types need different confidence cutoffs\n",
        "# ================================================================\n",
        "\n",
        "# Start with the global best threshold for all entities\n",
        "entity_thresholds = {label: best_threshold for label in label2id if label != 'O' and not label.startswith('I-')}\n",
        "\n",
        "# Entity types with lots of FPs need HIGHER thresholds\n",
        "# Entity types with lots of FNs and few FPs need LOWER thresholds\n",
        "# Based on your error analysis:\n",
        "HIGH_FP_ENTITIES = ['Kompatibles_Fahrzeug_Modell', 'Einbauposition', 'Produktart', 'Im_Lieferumfang_Enthalten']\n",
        "LOW_FP_ENTITIES  = ['Herstellernummer', 'Hersteller', 'Bremsscheibenart', 'Produktlinie']\n",
        "\n",
        "def get_word_level_predictions_per_entity(models, tokens, entity_thresholds, default_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Like get_word_level_predictions but uses per-entity thresholds.\n",
        "    \"\"\"\n",
        "    enc = tokenizer(\n",
        "        tokens,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    word_ids = enc.word_ids(batch_index=0)\n",
        "    input_ids = enc['input_ids'].to(DEVICE)\n",
        "    attn_mask  = enc['attention_mask'].to(DEVICE)\n",
        "\n",
        "    all_logits = []\n",
        "    for m in models:\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast('cuda'):  # updated API to fix FutureWarning\n",
        "                logits = m(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "        all_logits.append(logits.cpu().float())\n",
        "    avg_logits = torch.stack(all_logits).mean(0).squeeze(0)\n",
        "\n",
        "    probs = torch.softmax(avg_logits, dim=-1)\n",
        "    max_probs, pred_ids = probs.max(dim=-1)\n",
        "\n",
        "    word_preds = {}\n",
        "    for pos, wid in enumerate(word_ids):\n",
        "        if wid is None or wid in word_preds:\n",
        "            continue\n",
        "        label = id2label[pred_ids[pos].item()]\n",
        "        conf  = max_probs[pos].item()\n",
        "\n",
        "        if label == 'O':\n",
        "            word_preds[wid] = 'O'\n",
        "        else:\n",
        "            # Strip B- or I- to get entity type for threshold lookup\n",
        "            etype = label.replace('B-', '').replace('I-', '')\n",
        "            threshold = entity_thresholds.get(etype, default_threshold)\n",
        "            word_preds[wid] = label if conf >= threshold else 'O'\n",
        "\n",
        "    return [word_preds.get(i, 'O') for i in range(len(tokens))]\n",
        "\n",
        "\n",
        "# Tune each high-FP entity's threshold independently\n",
        "print('Tuning per-entity thresholds...')\n",
        "print('(Only tuning the problematic entities to save time)\\n')\n",
        "\n",
        "tuned_thresholds = dict(entity_thresholds)  # start from global best\n",
        "\n",
        "for entity in HIGH_FP_ENTITIES:\n",
        "    b_entity = f'B-{entity}'\n",
        "    if b_entity not in label2id:\n",
        "        continue\n",
        "\n",
        "    best_e_threshold = best_threshold\n",
        "    best_e_fbeta = 0.0\n",
        "\n",
        "    for t in np.arange(best_threshold, 0.97, 0.02):\n",
        "        test_thresholds = dict(tuned_thresholds)\n",
        "        test_thresholds[entity] = t\n",
        "\n",
        "        pred_spans_all = []\n",
        "        gold_spans_all_local = []\n",
        "        for ex in tune_examples:\n",
        "            bio_preds = get_word_level_predictions_per_entity(\n",
        "                fold_models, ex['tokens'], test_thresholds)\n",
        "            spans = bio_to_spans(ex['tokens'], bio_preds)\n",
        "            pred_spans_all.append([(t2, v) for t2, v in spans if t2 == entity])\n",
        "            gold_spans = bio_to_spans(ex['tokens'], ex['tags'])\n",
        "            gold_spans_all_local.append([(t2, v) for t2, v in gold_spans if t2 == entity])\n",
        "\n",
        "        p, r, f = compute_fbeta_score(pred_spans_all, gold_spans_all_local)\n",
        "        if f > best_e_fbeta:\n",
        "            best_e_fbeta = f\n",
        "            best_e_threshold = t\n",
        "\n",
        "    tuned_thresholds[entity] = best_e_threshold\n",
        "    print(f'  {entity}: {best_threshold:.2f} → {best_e_threshold:.2f} (fbeta={best_e_fbeta:.4f})')\n",
        "\n",
        "print('\\nFinal per-entity thresholds:')\n",
        "for entity, t in tuned_thresholds.items():\n",
        "    if t != best_threshold:\n",
        "        print(f'  {entity}: {t:.2f}  (changed from {best_threshold:.2f})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "NZiKsUCEtK_I",
        "outputId": "541faab1-abcf-4888-f582-8293ffbd4544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'label2id' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-264/427388335.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Start with the global best threshold for all entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mentity_thresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_threshold\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel2id\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'O'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Entity types with lots of FPs need HIGHER thresholds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'label2id' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3klud-jCiFN"
      },
      "source": [
        "## CELL 13 — Generate Quiz Submission\n",
        "This generates the submission file for records 5001-30000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QgWRjbL-CiFN",
        "outputId": "e3207762-4a07-482f-a136-4cebb86b3147"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tuned_thresholds' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-497/513249291.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0msub_path_v3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{OUTPUT_DIR}/submission_per_entity_thresholds.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdf_sub_v3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_submission_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquiz_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuned_thresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_path_v3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tuned_thresholds' is not defined"
          ]
        }
      ],
      "source": [
        "## CELL 13 — Generate Quiz Submission (Correct EvalAI Format)\n",
        "\n",
        "import datetime\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# In Cell 13, change generate_submission to use tuned_thresholds:\n",
        "\n",
        "def generate_submission_v2(examples, models, entity_thresholds, output_path):\n",
        "    print(f'Generating submission (per-entity thresholds)...')\n",
        "    rows = []\n",
        "    for i, ex in enumerate(examples):\n",
        "        if i % 2000 == 0:\n",
        "            print(f'  {i}/{len(examples)}')\n",
        "        bio_preds = get_word_level_predictions_per_entity(\n",
        "            models, ex['tokens'], entity_thresholds)\n",
        "        spans = bio_to_spans(ex['tokens'], bio_preds)\n",
        "        cat = ex['category_id']\n",
        "\n",
        "        for entity_type, aspect_value in spans:\n",
        "            if entity_type == 'O':\n",
        "               continue\n",
        "\n",
        "    # Filter invalid aspects for this category\n",
        "            if entity_type not in allowed_aspects_per_category[cat]:\n",
        "                continue\n",
        "\n",
        "            rows.append({\n",
        "                 'record_number': ex['record_id'],\n",
        "                 'category_id':   ex['category_id'],\n",
        "                 'aspect_name':   entity_type,\n",
        "                 'aspect_value':  aspect_value\n",
        "             })\n",
        "\n",
        "    df_sub = pd.DataFrame(rows)\n",
        "    import csv as csv_mod\n",
        "    df_sub.to_csv(output_path, sep='\\t', index=False,\n",
        "                  quoting=csv_mod.QUOTE_NONE, encoding='utf-8')\n",
        "    print(f'Saved: {output_path} ({len(df_sub)} rows)')\n",
        "    return df_sub\n",
        "\n",
        "sub_path_v3 = f'{OUTPUT_DIR}/submission_per_entity_thresholds.tsv'\n",
        "df_sub_v3 = generate_submission_v2(quiz_examples, fold_models, tuned_thresholds, sub_path_v3)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(sub_path_v3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGXvP26cCiFN"
      },
      "source": [
        "## CELL 14 — Generate Aggressive Threshold Submission\n",
        "Since F-beta=0.2 strongly rewards precision, this often scores higher on the leaderboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzHVJcO9CiFO",
        "outputId": "fd2d8312-fe72-4e65-9e01-11ddb11388d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating submission for 25000 examples...\n",
            "Threshold: 0.8500000000000003\n",
            "  Processing 0/25000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-623/4239980200.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing 2000/25000...\n",
            "  Processing 4000/25000...\n",
            "  Processing 6000/25000...\n",
            "  Processing 8000/25000...\n",
            "  Processing 10000/25000...\n",
            "  Processing 12000/25000...\n",
            "  Processing 14000/25000...\n",
            "  Processing 16000/25000...\n",
            "  Processing 18000/25000...\n",
            "  Processing 20000/25000...\n",
            "  Processing 22000/25000...\n",
            "  Processing 24000/25000...\n",
            "\n",
            "Saved: /content/drive/MyDrive/ebay_ner/outputs/submission_t0.85_1444.tsv\n",
            "Total rows: 159608\n",
            "Unique records with predictions: 24885\n",
            "\n",
            "v1 (t=0.79): 163692 rows (more recall)\n",
            "v2 (t=0.85): 159608 rows (more precision)\n",
            "Submit v1 first. If LB score disappoints, try v2.\n"
          ]
        }
      ],
      "source": [
        "sub_path_v2 = f'{OUTPUT_DIR}/submission_t{aggressive_threshold:.2f}_{ts}.tsv'\n",
        "df_sub_v2 = generate_submission(quiz_examples, fold_models, aggressive_threshold, sub_path_v2)\n",
        "\n",
        "print(f'\\nv1 (t={best_threshold:.2f}): {len(df_sub_v1)} rows (more recall)')\n",
        "print(f'v2 (t={aggressive_threshold:.2f}): {len(df_sub_v2)} rows (more precision)')\n",
        "print('Submit v1 first. If LB score disappoints, try v2.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOf71kvtCiFO"
      },
      "source": [
        "## CELL 15 — Validate & Download Submission Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "rs3vVNovCiFO",
        "outputId": "40eb1e8c-7e91-4fe4-b17b-9bd330a62dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Submission Validation ===\n",
            "OK: All required columns present\n",
            "OK: No null values\n",
            "OK: All record numbers are valid quiz IDs\n",
            "OK: No TAB characters in aspect values\n",
            "\n",
            "Total rows: 163692\n",
            "Sample rows:\n",
            "   record_number  category_id                  aspect_name          aspect_value\n",
            "0           5001            1    Kompatible_Fahrzeug_Marke                  OPEL\n",
            "1           5001            1  Kompatibles_Fahrzeug_Modell  ASTRA H 1.7 CDTI-SET\n",
            "2           5001            1         Anzahl_Der_Einheiten                     2\n",
            "3           5001            1    Im_Lieferumfang_Enthalten         Bremsscheiben\n",
            "4           5001            1         Anzahl_Der_Einheiten                     4\n",
            "5           5001            1    Im_Lieferumfang_Enthalten                Beläge\n",
            "6           5001            1               Einbauposition                    VA\n",
            "7           5002            1                   Produktart                  Satz\n",
            "8           5002            1    Im_Lieferumfang_Enthalten           Gabelfedern\n",
            "9           5002            1    Kompatible_Fahrzeug_Marke                   BMW\n",
            "\n",
            "Downloading submission files...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f1e9ef83-360f-435c-b8ed-3b040128f305\", \"submission_t0.79_1444.tsv\", 6161117)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9007a6b7-699f-4ced-8c4d-de45875dbfaa\", \"submission_t0.85_1444.tsv\", 5987178)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def validate_submission(df_sub, quiz_examples):\n",
        "    print('=== Submission Validation ===')\n",
        "\n",
        "    # 1. Required columns\n",
        "    required = ['record_number', 'category_id', 'aspect_name', 'aspect_value']\n",
        "    for col in required:\n",
        "        assert col in df_sub.columns, f'Missing column: {col}'\n",
        "    print('OK: All required columns present')\n",
        "\n",
        "    # 2. No null values\n",
        "    nulls = df_sub.isnull().sum().sum()\n",
        "    assert nulls == 0, f'{nulls} null values found!'\n",
        "    print('OK: No null values')\n",
        "\n",
        "    # 3. Record numbers are in range\n",
        "    valid_ids = set(ex['record_id'] for ex in quiz_examples)\n",
        "    sub_ids = set(df_sub['record_number'].astype(str))\n",
        "    invalid = sub_ids - valid_ids\n",
        "    if invalid:\n",
        "        print(f'WARNING: {len(invalid)} record numbers not in quiz set: {list(invalid)[:5]}')\n",
        "    else:\n",
        "        print(f'OK: All record numbers are valid quiz IDs')\n",
        "\n",
        "    # 4. Aspect values use only ASCII space (not tab)\n",
        "    has_tab = df_sub['aspect_value'].str.contains('\\t').any()\n",
        "    assert not has_tab, 'TAB found in aspect values!'\n",
        "    print('OK: No TAB characters in aspect values')\n",
        "\n",
        "    # 5. Show sample\n",
        "    print(f'\\nTotal rows: {len(df_sub)}')\n",
        "    print('Sample rows:')\n",
        "    print(df_sub.head(10).to_string())\n",
        "\n",
        "validate_submission(df_sub_v1, quiz_examples)\n",
        "\n",
        "# Download both files\n",
        "from google.colab import files\n",
        "print('\\nDownloading submission files...')\n",
        "files.download(sub_path_v1)\n",
        "files.download(sub_path_v2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXeyQ9t9CiFO"
      },
      "source": [
        "## CELL 16 — Quick Error Analysis on Validation Set (Optional)\n",
        "Run this to understand what your model gets wrong before submitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syocsvRUCiFO",
        "outputId": "44de49c8-88de-41af-884c-505e49d31e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-623/4239980200.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FALSE POSITIVES by type (over-predicted) ===\n",
            "  Kompatibles_Fahrzeug_Modell: 62\n",
            "  Produktart: 10\n",
            "  Einbauposition: 9\n",
            "  Im_Lieferumfang_Enthalten: 8\n",
            "  Herstellernummer: 3\n",
            "  Kompatible_Fahrzeug_Marke: 3\n",
            "  Kompatibles_Fahrzeug_Jahr: 2\n",
            "  Bremsscheiben-Aussendurchmesser: 2\n",
            "  Hersteller: 1\n",
            "\n",
            "=== FALSE NEGATIVES by type (under-predicted) ===\n",
            "  Kompatibles_Fahrzeug_Modell: 127\n",
            "  Produktart: 30\n",
            "  Im_Lieferumfang_Enthalten: 25\n",
            "  Einbauposition: 14\n",
            "  Herstellernummer: 10\n",
            "  Kompatible_Fahrzeug_Marke: 7\n",
            "  Hersteller: 5\n",
            "  Bremsscheibenart: 3\n",
            "  Produktlinie: 3\n",
            "  Kompatibles_Fahrzeug_Jahr: 3\n",
            "  Oe/Oem_Referenznummer(N): 3\n",
            "  Material: 2\n",
            "  Größe: 2\n",
            "  Besonderheiten: 2\n",
            "  Modell: 2\n",
            "  Breite: 1\n",
            "  Länge: 1\n",
            "  Stärke: 1\n",
            "  Bremsscheiben-Aussendurchmesser: 1\n",
            "  Menge: 1\n",
            "  Anwendung: 1\n",
            "\n",
            "--- Sample False Positives (wrong predictions) ---\n",
            "  Title: Bremsscheiben Bremsbeläge vorne hinten 5-Loch für Opel Astra G Zafira A 2.0 OPC\n",
            "  Incorrectly predicted: ('Kompatibles_Fahrzeug_Modell', 'Astra G')\n",
            "\n",
            "  Title: Bremsscheiben Bremsbeläge vorne hinten 5-Loch für Opel Astra G Zafira A 2.0 OPC\n",
            "  Incorrectly predicted: ('Einbauposition', 'vorne')\n",
            "\n",
            "  Title: NK Bremsscheiben + Beläge vorne + hinten für VW Golf 5 6 für AUDI A3 8P\n",
            "  Incorrectly predicted: ('Kompatibles_Fahrzeug_Modell', 'Golf')\n",
            "\n",
            "  Title: SKF Zahnriemensatz (VKMA 01113) für AUDI A3 VW Golf Plus V Polo III Jetta\n",
            "  Incorrectly predicted: ('Kompatibles_Fahrzeug_Modell', 'Polo III')\n",
            "\n",
            "  Title: SKF Zahnriemensatz (VKMA 01113) für AUDI A3 VW Golf Plus V Polo III Jetta\n",
            "  Incorrectly predicted: ('Kompatibles_Fahrzeug_Modell', 'Golf Plus V')\n",
            "\n",
            "--- Sample False Negatives (missed entities) ---\n",
            "  Title: Bremsscheiben Bremsbeläge vorne hinten 5-Loch für Opel Astra G Zafira A 2.0 OPC\n",
            "  Missed: ('Kompatibles_Fahrzeug_Modell', 'G')\n",
            "\n",
            "  Title: Bremsscheiben Bremsbeläge vorne hinten 5-Loch für Opel Astra G Zafira A 2.0 OPC\n",
            "  Missed: ('Bremsscheibenart', '5-Loch')\n",
            "\n",
            "  Title: Bremsscheiben Bremsbeläge vorne hinten 5-Loch für Opel Astra G Zafira A 2.0 OPC\n",
            "  Missed: ('Einbauposition', 'vorne hinten')\n",
            "\n",
            "  Title: Bremsscheiben Bremsbeläge vorne hinten 5-Loch für Opel Astra G Zafira A 2.0 OPC\n",
            "  Missed: ('Kompatibles_Fahrzeug_Modell', 'Astra')\n",
            "\n",
            "  Title: VAICO Bremsensatz, Scheibenbremse EXPERT KITS + Vorne V30-3680\n",
            "  Missed: ('Herstellernummer', 'V30-3680')\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "fp_types = Counter()\n",
        "fn_types = Counter()\n",
        "fp_examples = []\n",
        "fn_examples = []\n",
        "\n",
        "for ex in tune_examples[:200]:  # Sample 200 val examples\n",
        "    bio_preds = get_word_level_predictions(fold_models, ex['tokens'], best_threshold)\n",
        "    pred_spans = set(bio_to_spans(ex['tokens'], bio_preds))\n",
        "    gold_spans = set(bio_to_spans(ex['tokens'], ex['tags']))\n",
        "    pred_spans = {s for s in pred_spans if s[0] != 'O'}\n",
        "    gold_spans = {s for s in gold_spans if s[0] != 'O'}\n",
        "\n",
        "    for span in pred_spans - gold_spans:\n",
        "        fp_types[span[0]] += 1\n",
        "        if len(fp_examples) < 5:\n",
        "            fp_examples.append({'title': ex['title'], 'predicted': span})\n",
        "\n",
        "    for span in gold_spans - pred_spans:\n",
        "        fn_types[span[0]] += 1\n",
        "        if len(fn_examples) < 5:\n",
        "            fn_examples.append({'title': ex['title'], 'missed': span})\n",
        "\n",
        "print('=== FALSE POSITIVES by type (over-predicted) ===')\n",
        "for t, c in fp_types.most_common():\n",
        "    print(f'  {t}: {c}')\n",
        "\n",
        "print('\\n=== FALSE NEGATIVES by type (under-predicted) ===')\n",
        "for t, c in fn_types.most_common():\n",
        "    print(f'  {t}: {c}')\n",
        "\n",
        "print('\\n--- Sample False Positives (wrong predictions) ---')\n",
        "for x in fp_examples:\n",
        "    print(f'  Title: {x[\"title\"]}')\n",
        "    print(f'  Incorrectly predicted: {x[\"predicted\"]}\\n')\n",
        "\n",
        "print('--- Sample False Negatives (missed entities) ---')\n",
        "for x in fn_examples:\n",
        "    print(f'  Title: {x[\"title\"]}')\n",
        "    print(f'  Missed: {x[\"missed\"]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdCh4yawCiFO"
      },
      "source": [
        "## CELL 17 — OOM Fixes & Troubleshooting\n",
        "Run this cell if anything breaks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNFpY7_gCiFO"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PROBLEM: CUDA out of memory\n",
        "# FIX 1: Reduce batch size\n",
        "#   BATCH_SIZE = 8   (in Cell 10)\n",
        "#\n",
        "# FIX 2: Switch to smaller model\n",
        "#   MODEL_NAME = 'deepset/gbert-base'   (in Cell 7)\n",
        "#\n",
        "# FIX 3: Enable gradient checkpointing\n",
        "#   Add after model = load_fresh_model().to(DEVICE):\n",
        "#   model.gradient_checkpointing_enable()\n",
        "#\n",
        "# FIX 4: Clear memory between folds\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print('Memory cleared')\n",
        "print('GPU memory used:', round(torch.cuda.memory_allocated()/1e9,2), 'GB')\n",
        "print('GPU memory free:', round((torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated())/1e9,2), 'GB')\n",
        "\n",
        "# ============================================================\n",
        "# PROBLEM: Colab disconnected mid-training\n",
        "# FIX: Models are saved to Drive after each fold.\n",
        "# Re-run Cell 10 - already-saved folds just need to be reloaded:\n",
        "#\n",
        "# fold_models = []\n",
        "# for fold_num in range(1, N_FOLDS+1):\n",
        "#     m = load_fresh_model().to(DEVICE)\n",
        "#     m.load_state_dict(torch.load(f'{OUTPUT_DIR}/fold{fold_num}_best.pt'))\n",
        "#     m.eval()\n",
        "#     fold_models.append(m)\n",
        "#     print(f'Loaded fold {fold_num}')\n",
        "\n",
        "# ============================================================\n",
        "# PROBLEM: All predictions are 'O'\n",
        "# Your threshold is too high. Try threshold=0.3 first to confirm\n",
        "# the model is predicting anything at all, then tune up.\n",
        "\n",
        "# ============================================================\n",
        "# PROBLEM: Empty tag not reading correctly\n",
        "# Make sure you used keep_default_na=False, na_values=None in read_csv\n",
        "# Check: print(df_train_raw[COL_TAG].value_counts(dropna=False))\n",
        "\n",
        "print('See comments above for fixes.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL A: Fix Fahrzeug_Modell + generate better submission\n",
        "# Run RIGHT NOW\n",
        "\n",
        "import csv as csv_mod, datetime\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# --- Find best threshold for Fahrzeug_Modell ---\n",
        "print('Finding best Fahrzeug_Modell threshold...')\n",
        "print(f'{\"threshold\":>10} | {\"Precision\":>10} | {\"Recall\":>8} | {\"F-beta\":>10} | {\"# preds\":>8}')\n",
        "print('-'*58)\n",
        "\n",
        "best_modell_t     = 0.76\n",
        "best_modell_fbeta = 0.0\n",
        "\n",
        "for t_modell in np.arange(0.76, 0.97, 0.02):\n",
        "    test_t = dict(tuned_thresholds)\n",
        "    test_t['Kompatibles_Fahrzeug_Modell'] = t_modell\n",
        "\n",
        "    pred_spans_all   = []\n",
        "    gold_spans_local = []\n",
        "    for ex in tune_examples:\n",
        "        preds = get_word_level_predictions_per_entity(\n",
        "            fold_models, ex['tokens'], test_t)\n",
        "        spans = bio_to_spans(ex['tokens'], preds)\n",
        "        pred_spans_all.append(\n",
        "            [(t2, v) for t2, v in spans\n",
        "             if t2 == 'Kompatibles_Fahrzeug_Modell'])\n",
        "        gold = bio_to_spans(ex['tokens'], ex['tags'])\n",
        "        gold_spans_local.append(\n",
        "            [(t2, v) for t2, v in gold\n",
        "             if t2 == 'Kompatibles_Fahrzeug_Modell'])\n",
        "\n",
        "    p, r, f = compute_fbeta_score(pred_spans_all, gold_spans_local)\n",
        "    count   = sum(len(s) for s in pred_spans_all)\n",
        "    marker  = ' <-- BEST' if f > best_modell_fbeta else ''\n",
        "    print(f'{t_modell:>10.2f} | {p:>10.4f} | {r:>8.4f} | '\n",
        "          f'{f:>10.4f} | {count:>8}{marker}')\n",
        "\n",
        "    if f > best_modell_fbeta:\n",
        "        best_modell_fbeta = f\n",
        "        best_modell_t     = t_modell\n",
        "\n",
        "print(f'\\nBest threshold: {best_modell_t:.2f}')\n",
        "\n",
        "# Update threshold\n",
        "tuned_thresholds['Kompatibles_Fahrzeug_Modell'] = best_modell_t\n",
        "\n",
        "# --- Generate new submission ---\n",
        "rows = []\n",
        "for ex in quiz_examples:\n",
        "    category           = str(ex['category_id'])\n",
        "    valid_for_category = VALID_ASPECTS.get(category, set())\n",
        "    bio_preds          = get_word_level_predictions_per_entity(\n",
        "        fold_models, ex['tokens'], tuned_thresholds)\n",
        "    spans              = bio_to_spans(ex['tokens'], bio_preds)\n",
        "    for entity_type, aspect_value in spans:\n",
        "        if entity_type == 'O':\n",
        "            continue\n",
        "        if entity_type not in valid_for_category:\n",
        "            continue\n",
        "        rows.append({\n",
        "            'record_number': ex['record_id'],\n",
        "            'category_id':   ex['category_id'],\n",
        "            'aspect_name':   entity_type,\n",
        "            'aspect_value':  aspect_value\n",
        "        })\n",
        "\n",
        "df_new   = pd.DataFrame(rows)\n",
        "ts       = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
        "new_path = f'{OUTPUT_DIR}/submission_modell_fix_{ts}.tsv'\n",
        "df_new.to_csv(new_path, sep='\\t', index=False,\n",
        "              quoting=csv_mod.QUOTE_NONE, encoding='utf-8')\n",
        "\n",
        "print(f'Old Fahrzeug_Modell count: 35,509')\n",
        "print(f'New Fahrzeug_Modell count: '\n",
        "      f'{(df_new[\"aspect_name\"]==\"Kompatibles_Fahrzeug_Modell\").sum():,}')\n",
        "print(f'Total rows: {len(df_new):,}')\n",
        "\n",
        "from google.colab import files\n",
        "files.download(new_path)\n",
        "print('\\nDone! Submit this file to EvalAI now.')"
      ],
      "metadata": {
        "id": "r2kMroXbsZhw",
        "outputId": "9ee66a3b-a6c6-4910-c8b8-6420e9c6a706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding best Fahrzeug_Modell threshold...\n",
            " threshold |  Precision |   Recall |     F-beta |  # preds\n",
            "----------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tuned_thresholds' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-295/4122275643.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt_modell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.76\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.97\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtest_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuned_thresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtest_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Kompatibles_Fahrzeug_Modell'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_modell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tuned_thresholds' is not defined"
          ]
        }
      ]
    }
  ]
}